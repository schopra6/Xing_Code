{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed to reproduce results with tf and keras\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(2)\n",
    "# other imports\n",
    "from keras.layers import Input, Dense, multiply, Lambda\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "path =  \"../../data/\"\n",
    "dataset = \"recsys17/\"\n",
    "\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\"\n",
    "\n",
    "\n",
    "valid_train = pd.read_csv(processed_path + \"valid_train_14d.csv\", header=0, sep='\\t')\n",
    "valid_test = pd.read_csv(processed_path + \"valid_test_14d.csv\", header=0, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "unqiue_train_items_df = pd.read_csv(interim_path + 'vocabulary.csv', index_col=0)\n",
    "# create dictionary out of it\n",
    "unqiue_train_items_dict = unqiue_train_items_df.to_dict('dict')[\"item_id\"]\n",
    "# inverse that item_id is key and index is value\n",
    "unqiue_train_items_dict_inv = {v: k for k, v in unqiue_train_items_dict.items()}\n",
    "print(\"Vocabulary size: \" + str(len(unqiue_train_items_dict_inv)))\n",
    "\n",
    "# encode data\n",
    "def encode_data(session_df, unqiue_train_items_dict_inv):\n",
    "    session_vectors = []\n",
    "    session_groups = session_df.groupby(\"session_id\")\n",
    "    print(str(len(session_groups)) + \" sessions to encode.\")\n",
    "    \n",
    "    s_counter = 0      \n",
    "    for session_id, session_group in session_groups:\n",
    "        # vector length = len(unqiue_train_items)\n",
    "        session_vector = np.zeros((len(unqiue_train_items_dict_inv),), dtype=int)\n",
    "        # fill 1s for session items\n",
    "        for index, row in session_group.iterrows():\n",
    "            item_index = unqiue_train_items_dict_inv[row[\"item_id\"]]\n",
    "            # 1-hot encode\n",
    "            session_vector[item_index] = 1\n",
    "            \n",
    "        session_vectors.append(session_vector)\n",
    "        #session_vectors = np.concatenate(([session_vectors, np.array([session_vector])]), axis=0)\n",
    "        #session_vectors = np.vstack([session_vectors, session_vector])\n",
    "        s_counter += 1\n",
    "        if (s_counter % 1000 == 0):\n",
    "            print(str(len(session_groups) - s_counter) + \" sessions remaining to encode.\")\n",
    "\n",
    "    return np.vstack(session_vectors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_valid_train = encode_data(valid_train, unqiue_train_items_dict_inv)\n",
    "enc_valid_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_valid_test = encode_data(valid_test, unqiue_train_items_dict_inv)\n",
    "enc_valid_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define (Denoising) Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ae(model_path, input_size, train_set, valid_set, encoding_dim = 100, hidden_dim = 256, attention=False, dae=False):\n",
    "    # this is our input placeholder; size of vocabulary\n",
    "    input_vec = Input(shape=(input_size, ))\n",
    "    \n",
    "    l2_reg = regularizers.l2(1e-4)\n",
    "    \n",
    "    #encoded = input_vec\n",
    "    #if attention == True:\n",
    "        # ATTENTION PART STARTS HERE\n",
    "    #    attention_probs = Dense(input_size, activation='softmax', name='attention_vec')(input_vec)\n",
    "    #    encoded = multiply([input_vec, attention_probs], name='attention_mul')\n",
    "        # ATTENTION PART FINISHES HERE\n",
    "\n",
    "        \n",
    "    # \"encoded\" is the encoded representation of the inputs\n",
    "    encoded = Dense(hidden_dim, activation='relu', activity_regularizer=l2_reg)(input_vec)\n",
    "    \n",
    "    if attention == True:\n",
    "        # ATTENTION PART STARTS HERE\n",
    "        attention_probs = Dense(hidden_dim, activation='softmax', name='attention_vec')(encoded)\n",
    "        encoded = multiply([encoded, attention_probs], name='attention_mul')\n",
    "        # ATTENTION PART FINISHES HERE\n",
    "\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "    # \"decoded\" is the lossy reconstruction of the input\n",
    "    decoded = Dense(hidden_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(input_size, activation='sigmoid')(decoded)\n",
    "\n",
    "    # this model maps an input to its reconstruction\n",
    "    autoencoder = Model(input_vec, decoded)\n",
    "\n",
    "    # Separate Encoder model\n",
    "    # this model maps an input to its encoded representation\n",
    "    encoder = Model(input_vec, encoded)\n",
    "\n",
    "    # Separate Decoder model\n",
    "    # create a placeholder for an encoded (32-dimensional) input\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    # retrieve the last layer of the autoencoder model\n",
    "    decoder_layer1 = autoencoder.layers[-2]\n",
    "    decoder_layer2 = autoencoder.layers[-1]\n",
    "    \n",
    "    # create the decoder model\n",
    "    #decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "    decoder = Model(encoded_input, decoder_layer2(decoder_layer1(encoded_input)))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Set callback functions to early stop training and save the best model so far\n",
    "    model_name = model_path\n",
    "    if attention == True:\n",
    "        model_name = model_name + \"att_\"\n",
    "        \n",
    "    if dae == True:\n",
    "        model_name = model_name + 'dae-{epoch:02d}.model'\n",
    "    else:\n",
    "        model_name = model_name + 'ae-{epoch:02d}.model'\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    mc = ModelCheckpoint(filepath=model_name, \n",
    "                         monitor='val_loss', \n",
    "                         save_best_only=False, \n",
    "                         save_weights_only=False, \n",
    "                         mode='auto', \n",
    "                         period=1)\n",
    "    callbacks = [es, mc]\n",
    "                    \n",
    "    # configure model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer\n",
    "    opt = optimizers.RMSprop(lr=0.01, rho=0.9, epsilon=None, decay=0.0)\n",
    "    autoencoder.compile(optimizer=opt, loss='kullback_leibler_divergence')\n",
    "\n",
    "    train_in = train_set\n",
    "    valid_in = valid_set\n",
    "    if dae == True:\n",
    "        noise_factor = 0.5\n",
    "        train_in = train_set + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=train_set.shape)\n",
    "        valid_in = valid_set + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=valid_set.shape)\n",
    "        train_in = np.clip(train_in, 0., 1.)\n",
    "        valid_in = np.clip(valid_in, 0., 1.)\n",
    "\n",
    "    autoencoder.fit(train_in, train_set, \n",
    "                    callbacks=callbacks, # Early stopping \n",
    "                    epochs=50, \n",
    "                    batch_size=256, \n",
    "                    shuffle=True, \n",
    "                    validation_data=(valid_in, valid_set), \n",
    "                    verbose=2)\n",
    "    \n",
    "    return encoder, decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train normal Autoencoder\n",
    "\n",
    "* Set **attention = False** to train **without** the attention mechanism\n",
    "* Set **attention = True** to train **with** the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = False\n",
    "model_path = interim_path + \"models/\"\n",
    "vocab_size = len(unqiue_train_items_dict_inv)  # size of vocab\n",
    "encoder, decoder = train_ae(model_path, vocab_size, enc_valid_train, enc_valid_test, attention = attention)\n",
    "\n",
    "print(\"Finished training. Storing model to: \" + interim_path)\n",
    "model_name = \"ae_\"\n",
    "if attention == True:\n",
    "    model_name = \"att_\" + model_name\n",
    "pickle.dump(encoder, open(model_path + model_name + \"encoder.model\", 'wb'), protocol=4)\n",
    "pickle.dump(decoder, open(model_path + model_name + \"decoder.model\", 'wb'), protocol=4)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Denoising Autoencoder\n",
    "\n",
    "* Set **attention = False** to train **without** the attention mechanism\n",
    "* Set **attention = True** to train **with** the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = False\n",
    "model_path = interim_path + \"models/\"\n",
    "vocab_size = len(unqiue_train_items_dict_inv)  # size of vocab\n",
    "dae_encoder, dae_decoder = train_ae(model_path, vocab_size, enc_valid_train, enc_valid_test, dae = True, attention = attention)\n",
    "\n",
    "print(\"Finished training. Storing model to: \" + interim_path)\n",
    "model_name = \"dae_\"\n",
    "if attention == True:\n",
    "    model_name = \"att_\" + model_name\n",
    "pickle.dump(dae_encoder, open(model_path + model_name + \"encoder.model\", 'wb'), protocol=4)\n",
    "pickle.dump(dae_decoder, open(model_path + model_name + \"decoder.model\", 'wb'), protocol=4)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reparameterization trick\n",
    "# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n",
    "# z = z_mean + sqrt(var) * epsilon\n",
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    \n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def train_vae(model_path, input_size, \n",
    "             train_set, valid_set, \n",
    "             encoding_dim = 100, hidden_dim = 256, \n",
    "             attention=False):\n",
    "    # VAE model = encoder + decoder\n",
    "    # build encoder model\n",
    "    x = Input(shape=(input_size, ), name='encoder_input')\n",
    "    h = Dense(hidden_dim, activation='relu')(x)\n",
    "    \n",
    "    if attention == True:\n",
    "        # ATTENTION PART STARTS HERE\n",
    "        attention_probs = Dense(hidden_dim, activation='softmax', name='attention_vec')(h)\n",
    "        h = multiply([h, attention_probs], name='attention_mul')\n",
    "        # ATTENTION PART FINISHES HERE\n",
    "        \n",
    "    z_mean = Dense(encoding_dim, name='z_mean')(h)\n",
    "    z_log_var = Dense(encoding_dim, name='z_log_var')(h)\n",
    "    \n",
    "    # use reparameterization trick to push the sampling out as input\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, name='z')([z_mean, z_log_var])\n",
    "    \n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense(hidden_dim, activation='relu')\n",
    "    decoder_mean = Dense(input_size, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "    # instantiate VAE model\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "\n",
    "    # Compute VAE loss\n",
    "    xent_loss = input_size * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    vae_loss = K.mean(xent_loss + kl_loss)\n",
    "\n",
    "    vae.add_loss(vae_loss)\n",
    "    vae.compile(optimizer='rmsprop')\n",
    "    vae.summary()\n",
    "    \n",
    "    \n",
    "    model_name = model_path + 'vae-{epoch:02d}.model'\n",
    "    es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "    mc = ModelCheckpoint(filepath=model_name, \n",
    "                         monitor='val_loss', \n",
    "                         save_best_only=False, \n",
    "                         save_weights_only=False, \n",
    "                         mode='auto', \n",
    "                         period=1)\n",
    "    callbacks = [es, mc]\n",
    "    \n",
    "    vae.fit(train_set,\n",
    "        shuffle=True,\n",
    "        epochs=50,\n",
    "        callbacks=callbacks, # Early stopping \n",
    "        batch_size=256,\n",
    "        verbose=2,\n",
    "        validation_data=(valid_set, None))\n",
    "    \n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "    \n",
    "    # build a digit generator that can sample from the learned distribution\n",
    "    decoder_input = Input(shape=(encoding_dim,))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    return encoder, generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Variational Autoencoder\n",
    "\n",
    "* Set **attention = False** to train **without** the attention mechanism\n",
    "* Set **attention = True** to train **with** the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = False\n",
    "model_path = interim_path + \"models/\"\n",
    "vocab_size = len(unqiue_train_items_dict_inv)  # size of vocab\n",
    "vae_encoder, vae_decoder = train_vae(model_path, vocab_size, enc_valid_train, enc_valid_test, attention = attention)\n",
    "\n",
    "print(\"Finished training. Storing model to: \" + interim_path)\n",
    "model_name = \"vae_\"\n",
    "if attention == True:\n",
    "    model_name = \"att_\" + model_name\n",
    "pickle.dump(vae_encoder, open(model_path + model_name + \"encoder.model\", 'wb'), protocol=4)\n",
    "pickle.dump(vae_decoder, open(model_path + model_name + \"decoder.model\", 'wb'), protocol=4)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer test session vectors\n",
    "\n",
    "Every session will be divided into sub-sessions for which the latent vector will be inferred\n",
    "\n",
    "\n",
    "* Set **dae = False** to infer latent session vectors using the trained **Autoencoder**\n",
    "* Set **dae = True** to infer latent session vectors using the trained **Denoising Autoencoder**\n",
    "\n",
    "\n",
    "* Set **attention = False** to infer latent session vectors using the model that was trained **without** the attention mechanism\n",
    "* Set **attention = True** to infer latent session vectors using the model that was trained **with** the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"vae\" # ae | dae | vae\n",
    "attention = False\n",
    "\n",
    "test = pd.read_csv(processed_path + \"test_14d.csv\", header=0, sep='\\t')\n",
    "test_session_groups = test.groupby(\"session_id\")\n",
    "print(str(len(test_session_groups)) + \" sessions to test.\")\n",
    "    \n",
    "model_to_load = algo + \"_encoder.model\"\n",
    "if attention == True:\n",
    "    model_to_load = \"att_\" + model_to_load\n",
    "    \n",
    "encoder = pickle.load(open(model_path + model_to_load, 'rb'))\n",
    "\n",
    "    \n",
    "infer_columns = [\"session_id\", \"latent_session_vector\", \"input_items\", \"remaining_items\"]\n",
    "infer_df = pd.DataFrame(columns = infer_columns)\n",
    "    \n",
    "s_counter = 0\n",
    "for session_id, session_group in test_session_groups:\n",
    "    # vector length = len(unqiue_train_items)\n",
    "    session_vector = np.zeros((vocab_size,), dtype=int)\n",
    "    # fill 1s for session items\n",
    "    max_index = len(session_group) - 1\n",
    "    item_counter = 0\n",
    "    for index, row in session_group.iterrows():\n",
    "        # go from item to item \n",
    "        # leave last one for testing\n",
    "        if item_counter == max_index:\n",
    "            break\n",
    "        # make prediction\n",
    "        item_index = unqiue_train_items_dict_inv[row[\"item_id\"]]\n",
    "        session_vector[item_index] = 1\n",
    "        test_in = session_vector.reshape((1, vocab_size))\n",
    "        latent_session_vec = encoder.predict(test_in)\n",
    "        \n",
    "        # get input and remaining (expected) items\n",
    "        current_input_set = session_group[\"item_id\"].values[:item_counter+1]\n",
    "        remaining_test_set = session_group[\"item_id\"].values[item_counter+1:]\n",
    "        item_counter += 1\n",
    "\n",
    "        infer_df = infer_df.append({\n",
    "            \"session_id\": session_id,\n",
    "            \"latent_session_vector\": ','.join(map(str, latent_session_vec[0])),\n",
    "            \"input_items\":  ','.join(map(str, current_input_set)),\n",
    "            \"remaining_items\":  ','.join(map(str, remaining_test_set))\n",
    "        }, ignore_index=True)\n",
    "    s_counter += 1\n",
    "    if (s_counter % 1000 == 0):\n",
    "        print(str(len(test_session_groups) - s_counter) + \" test sessions remaining to infer latent vector.\")\n",
    "\n",
    "infer_path = interim_path + \"infer/\"\n",
    "infer_file_name = algo + \"_encoder_test.csv\"\n",
    "\n",
    "if attention == True:\n",
    "    infer_file_name = \"att_\" + infer_file_name\n",
    "infer_df.to_csv(infer_path + infer_file_name, sep='\\t', header=False, index=False)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = \"vae\" # ae | dae | vae\n",
    "attention = True\n",
    "\n",
    "train = pd.read_csv(processed_path + \"train_14d.csv\", header=0, sep='\\t')\n",
    "train_session_groups = train.groupby(\"session_id\")\n",
    "print(str(len(train_session_groups)) + \" sessions to test.\")\n",
    "    \n",
    "model_to_load = algo + \"_encoder.model\"\n",
    "if attention == True:\n",
    "    model_to_load = \"att_\" + model_to_load\n",
    "    \n",
    "encoder = pickle.load(open(model_path + model_to_load, 'rb'))\n",
    "    \n",
    "infer_columns = [\"session_id\", \"latent_session_vector\", \"items\"]\n",
    "infer_df = pd.DataFrame(columns = infer_columns)\n",
    "    \n",
    "s_counter = 0\n",
    "for session_id, session_group in train_session_groups:\n",
    "    # vector length = len(unqiue_train_items)\n",
    "    session_vector = np.zeros((vocab_size,), dtype=int)\n",
    "    # init train session vector\n",
    "    for index, row in session_group.iterrows():\n",
    "        item_index = unqiue_train_items_dict_inv[row[\"item_id\"]]\n",
    "        session_vector[item_index] = 1\n",
    "        \n",
    "    test_in = session_vector.reshape((1, vocab_size))\n",
    "    latent_session_vec = encoder.predict(test_in)\n",
    "\n",
    "    infer_df = infer_df.append({\n",
    "        \"session_id\": session_id,\n",
    "        \"latent_session_vector\": ','.join(map(str, latent_session_vec[0])),\n",
    "        \"items\":  ','.join(map(str, session_group[\"item_id\"].values)),\n",
    "    }, ignore_index=True)\n",
    "    s_counter += 1\n",
    "    if (s_counter % 1000 == 0):\n",
    "        print(str(len(train_session_groups) - s_counter) + \" train sessions remaining to infer latent vector.\")\n",
    "\n",
    "infer_path = interim_path + \"infer/\"\n",
    "infer_file_name = algo + \"_encoder_train.csv\"\n",
    "\n",
    "if attention == True:\n",
    "    infer_file_name = \"att_\" + infer_file_name\n",
    "    \n",
    "infer_df.to_csv(infer_path + infer_file_name, sep='\\t', header=False, index=False)\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
