{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing RecSys 2017\n",
    "\n",
    "For the RecSys 2017 dataset we first need to artificially create sessions out of the user internactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sessions(data, \n",
    "                  session_th=30 * 60, \n",
    "                  is_ordered=False, \n",
    "                  user_key='user_id', \n",
    "                  item_key='item_id', \n",
    "                  time_key='ts'):\n",
    "    \"\"\"Assigns session ids to the events in data without grouping keys\"\"\"\n",
    "    if not is_ordered:\n",
    "        # sort data by user and time\n",
    "        data.sort_values(by=[user_key, time_key], ascending=True, inplace=True)\n",
    "    # compute the time difference between queries\n",
    "    tdiff = np.diff(data[time_key].values)\n",
    "    # check which of them are bigger then session_th\n",
    "    split_session = tdiff > session_th\n",
    "    split_session = np.r_[True, split_session]\n",
    "    # check when the user chenges is data\n",
    "    new_user = data['user_id'].values[1:] != data['user_id'].values[:-1]\n",
    "    new_user = np.r_[True, new_user]\n",
    "    # a new sessions stars when at least one of the two conditions is verified\n",
    "    new_session = np.logical_or(new_user, split_session)\n",
    "    # compute the session ids\n",
    "    session_ids = np.cumsum(new_session)\n",
    "    data['session_id'] = session_ids\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test set\n",
    "\n",
    "A test set can be either created by (1) adding the last session of every user to be tested or, (2) making a time-based split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_session_out_split(data,\n",
    "                           user_key='user_id',\n",
    "                           item_key='item_id',\n",
    "                           session_key='session_id',\n",
    "                           time_key='ts',\n",
    "                           clean_test=True,\n",
    "                           min_session_length=2):\n",
    "    \"\"\"\n",
    "    last-session-out split\n",
    "    assign the last session of every user to the test set and the remaining ones to the training set\n",
    "    \"\"\"\n",
    "    sessions = data.sort_values(by=[user_key, time_key]).groupby(user_key)[session_key]\n",
    "    last_session = sessions.last()\n",
    "    train = data[~data.session_id.isin(last_session.values)].copy()\n",
    "    test = data[data.session_id.isin(last_session.values)].copy()\n",
    "    if clean_test:\n",
    "        train_items = train[item_key].unique()\n",
    "        test = test[test[item_key].isin(train_items)]\n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        slen = test[session_key].value_counts()\n",
    "        good_sessions = slen[slen >= min_session_length].index\n",
    "        test = test[test[session_key].isin(good_sessions)].copy()\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_n_days_out_split(data, n=1,\n",
    "                          user_key='user_id',\n",
    "                          item_key='item_id',\n",
    "                          session_key='session_id',\n",
    "                          time_key='ts',\n",
    "                          clean_test=True,\n",
    "                          min_session_length=2):\n",
    "    \"\"\"\n",
    "    last n-days out split\n",
    "    assign the sessions in the last n days to the test set and remaining to the training one\n",
    "    \"\"\"\n",
    "    DAY = 24 * 60 * 60\n",
    "    data.sort_values(by=[user_key, time_key], inplace=True)\n",
    "    # start times of all sessions\n",
    "    #sessions_start = data.groupby(session_key)[time_key].agg('min')\n",
    "    # extract test start and end time\n",
    "    end_time = data[time_key].max()\n",
    "    test_start = end_time - n * DAY\n",
    "    \n",
    "    # get train and test indicies\n",
    "    session_max_times = data.groupby(session_key)[time_key].max()\n",
    "    session_train = session_max_times[session_max_times < test_start].index\n",
    "    session_test = session_max_times[session_max_times >= test_start].index\n",
    "    \n",
    "    # in1d: Returns a boolean array the same length as ar1 that is True where \n",
    "    # an element of ar1 is in ar2 and False otherwise.\n",
    "    train = data[\n",
    "        np.in1d(\n",
    "            data[session_key], \n",
    "            session_train\n",
    "        )\n",
    "    ].copy()\n",
    "    test = data[\n",
    "        np.in1d(\n",
    "            data[session_key], \n",
    "            session_test\n",
    "        )\n",
    "    ].copy()\n",
    "\n",
    "    #train = data[data.session_id.isin(sessions_start[sessions_start < test_start].index)].copy()\n",
    "    #test = data[data.session_id.isin(sessions_start[sessions_start >= test_start].index)].copy()\n",
    "    if clean_test:\n",
    "        before_items = len(test[item_key].unique())\n",
    "        # remove items which do not occur in the test set\n",
    "        test = test[np.in1d(test[item_key], train[item_key])]\n",
    "        after_items = len(test[item_key].unique())\n",
    "        print(\"Before item count: \" + str(before_items))\n",
    "        print(\"After item count: \" + str(after_items))\n",
    "        \n",
    "        #  remove sessions in test shorter than min_session_length\n",
    "        \n",
    "        tslength = test.groupby(session_key).size()\n",
    "        test = test[\n",
    "           np.in1d(\n",
    "                test[session_key], \n",
    "                tslength[tslength >= min_session_length].index\n",
    "            )\n",
    "        ].copy()\n",
    "    \n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  1. RecSys17 processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path =  \"../../data/\"\n",
    "dataset = \"recsys17/\"\n",
    "\n",
    "raw_path = path + dataset + \"raw/\" \n",
    "interim_path = path + dataset + \"interim/\"\n",
    "processed_path = path + dataset + \"processed/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the RecSys17 dataset, we:\n",
    "* Remove the **delete recommendation** and **recruiter interest** interactions as these are not relevant in our setting\n",
    "* **Discard** the **impression interaction** as these denote that XING showed\n",
    "the corresponding job to a user. As stated by Bianchi, et al., 2017, the **presence of an impression does not imply** that the **user interacted with the job** and would thus **introduce bias** and possibly lead to learning a model that mimics XINGs recommender engine\n",
    "\n",
    "Sessions are partitioned by a **30-minute** idle time\n",
    "\n",
    "Keep all sessions: users with >= 2 sessions and also overly active ones (< 200,000 sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Time: 2016-11-06 09:19:02\n",
      "Start Time: 2017-02-07 20:29:23\n",
      "Building sessions\n",
      "        user_id  item_id  interaction_type  created_at  session_id\n",
      "25654        13   118310                 1  1484211749           1\n",
      "630641       13  1875610                 1  1486027147           2\n",
      "630640       13  1875610                 2  1486042146           3\n",
      "Original data:\n",
      "Num items: 51147\n",
      "Num users: 249987\n",
      "Num sessions: 456661\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 20135\n",
      "Num users: 13369\n",
      "Num sessions: 18284\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(raw_path + \"interactions.csv\", header=0, sep='\\t')\n",
    "print(\"Start Time: {}\".format(pd.to_datetime(interactions[\"created_at\"].min(), unit=\"s\")))\n",
    "print(\"Start Time: {}\".format(pd.to_datetime(interactions[\"created_at\"].max(), unit=\"s\")))\n",
    "\n",
    "# remove NaN values (should have only 1)\n",
    "interactions = interactions[np.isfinite(interactions['created_at'])]\n",
    "# convert back to long from float\n",
    "interactions['created_at'] = interactions['created_at'].astype(np.int64)\n",
    "\n",
    "# remove impressions\n",
    "interactions = interactions[interactions.interaction_type >= 1].copy()\n",
    "# remove delete and headhunter event types\n",
    "interactions = interactions[interactions.interaction_type < 4].copy()\n",
    "\n",
    "\n",
    "interactions['interaction_type'] = interactions['interaction_type'].fillna(0).astype('int')\n",
    "\n",
    "\n",
    "print('Building sessions')\n",
    "# partition interactions into sessions with 30-minutes idle time\n",
    "interactions = make_sessions(interactions, session_th=30 * 60, time_key='created_at', is_ordered=False)\n",
    "\n",
    "\n",
    "print(interactions.head(3))\n",
    "# drop 189 duplicate interactions\n",
    "interactions = interactions.drop_duplicates(['session_id','created_at'])\n",
    "\n",
    "print('Original data:')\n",
    "print('Num items: {}'.format(interactions.item_id.nunique()))\n",
    "print('Num users: {}'.format(interactions.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(interactions.session_id.nunique()))\n",
    "\n",
    "print('Filtering data')\n",
    "# drop duplicate interactions within the same session\n",
    "interactions.drop_duplicates(subset=['item_id', 'session_id', 'interaction_type'], keep='first', inplace=True)\n",
    "\n",
    "# keep items with >=1 interactions\n",
    "item_pop = interactions.item_id.value_counts()\n",
    "good_items = item_pop[item_pop >= 1].index\n",
    "inter_dense = interactions[interactions.item_id.isin(good_items)]\n",
    "\n",
    "# remove sessions with length < 2\n",
    "session_length = inter_dense.session_id.value_counts()\n",
    "good_sessions = session_length[session_length >= 3].index\n",
    "inter_dense = inter_dense[inter_dense.session_id.isin(good_sessions)]\n",
    "\n",
    "# let's keep only returning users (with >= 5 sessions) and remove overly active ones (>=200 sessions)\n",
    "sess_per_user = inter_dense.groupby('user_id')['session_id'].nunique()\n",
    "good_users = sess_per_user[(sess_per_user >= 1) & (sess_per_user < 200000)].index\n",
    "inter_dense = inter_dense[inter_dense.user_id.isin(good_users)]\n",
    "print('Filtered data:')\n",
    "print('Num items: {}'.format(inter_dense.item_id.nunique()))\n",
    "print('Num users: {}'.format(inter_dense.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(inter_dense.session_id.nunique()))\n",
    "\n",
    "inter_dense.to_csv(interim_path + \"interactions.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create train and test set by doing a time-based (2 weeks) split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioning data\n",
      "Before item count: 6789\n",
      "After item count: 2340\n",
      "Before item count: 7605\n",
      "After item count: 1758\n"
     ]
    }
   ],
   "source": [
    "print('Partitioning data')\n",
    "# last-session-out partitioning\n",
    "train_full_sessions, test_sessions = last_n_days_out_split(inter_dense, n=14,\n",
    "                                                            user_key='user_id',\n",
    "                                                            item_key='item_id',\n",
    "                                                            session_key='session_id',\n",
    "                                                            time_key='created_at',\n",
    "                                                            clean_test=True)\n",
    "train_valid_sessions, valid_sessions = last_n_days_out_split(train_full_sessions, n=14,\n",
    "                                                              user_key='user_id',\n",
    "                                                              item_key='item_id',\n",
    "                                                              session_key='session_id',\n",
    "                                                              time_key='created_at',\n",
    "                                                              clean_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training - Sessions: 12712\n",
      "Testing - Sessions: 3610\n",
      "Train + Test - Sessions: 16322\n",
      "Training - Items: 15686\n",
      "Testing - Items: 3610\n",
      "Train + Test - Items: 15686\n",
      "Train Validating - Sessions: 8001\n",
      "Test Validating - Sessions: 2046\n"
     ]
    }
   ],
   "source": [
    "# print statistics\n",
    "\n",
    "train_len = len(train_full_sessions.session_id.unique())\n",
    "train_item_len = len(train_full_sessions.item_id.unique())\n",
    "\n",
    "test_len = len(test_sessions.session_id.unique())\n",
    "test_item_len = len(test_sessions.item_id.unique())\n",
    "\n",
    "merged_items = train_full_sessions.append(test_sessions, ignore_index=True)\n",
    "merged_item_len = len(merged_items.item_id.unique())\n",
    "\n",
    "print(\"Training - Sessions: \" + str(train_len))\n",
    "print(\"Testing - Sessions: \" + str(test_len))\n",
    "print(\"Train + Test - Sessions: \" + str(train_len + test_len))\n",
    "\n",
    "print(\"Training - Items: \" + str(train_item_len))\n",
    "print(\"Testing - Items: \" + str(test_len))\n",
    "print(\"Train + Test - Items: \" + str(merged_item_len))\n",
    "\n",
    "\n",
    "print(\"Train Validating - Sessions: \" + str(len(train_valid_sessions.session_id.unique())))\n",
    "print(\"Test Validating - Sessions: \" + str(len(valid_sessions.session_id.unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Store train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_sessions.to_csv(processed_path + \"train_14d.csv\", sep='\\t')\n",
    "test_sessions.to_csv(processed_path + \"test_14d.csv\", sep='\\t')\n",
    "train_valid_sessions.to_csv(processed_path + \"valid_train_14d.csv\", sep='\\t')\n",
    "valid_sessions.to_csv(processed_path + \"valid_test_14d.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create train and test session vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15686\n",
      "0\n",
      "12712 sessions to encode.\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary from train set\n",
    "unqiue_train_items = train_full_sessions.item_id.unique()\n",
    "# store (or load)\n",
    "unqiue_train_items_df = pd.DataFrame(unqiue_train_items, columns=[\"item_id\"])\n",
    "print(len(unqiue_train_items_df))\n",
    "unqiue_train_items_df.to_csv(interim_path + 'vocabulary.csv', header=True)\n",
    "unqiue_train_items_df = pd.read_csv(interim_path + 'vocabulary.csv', index_col=0)\n",
    "\n",
    "\n",
    "unqiue_train_items_dict = unqiue_train_items_df.to_dict('dict')[\"item_id\"]\n",
    "# inverse that item_id is key and index is value\n",
    "unqiue_train_items_dict_inv = {v: k for k, v in unqiue_train_items_dict.items()}\n",
    "print(unqiue_train_items_dict_inv[864950])\n",
    "\n",
    "# session_vectors = []\n",
    "session_vectors_np = []\n",
    "session_groups = train_full_sessions.groupby(\"session_id\")\n",
    "\n",
    "print(str(len(session_groups)) + \" sessions to encode.\")\n",
    "s_counter = 0      \n",
    "for session_id, session_group in session_groups:\n",
    "    # vector length = len(unqiue_train_items)\n",
    "    session_vector = np.zeros((len(unqiue_train_items),), dtype=int)\n",
    "    # fill 1s for session items\n",
    "    for index, row in session_group.iterrows():\n",
    "        item_index = unqiue_train_items_dict_inv[row[\"item_id\"]]\n",
    "        #item_index = unqiue_train_items.index(row[\"item_id\"])\n",
    "        # 1-hot encode\n",
    "        session_vector[item_index] = 1\n",
    "        #break\n",
    "    # append session vector\n",
    "#     session_vectors.append(session_vector)\n",
    "    session_vectors_np.append(np.insert(session_vector, 0, s_counter))\n",
    "    s_counter += 1\n",
    "    if (s_counter % 10000 == 0):\n",
    "        print(str(len(session_groups) - s_counter) + \" sessions remaining to encode.\")\n",
    "\n",
    "# session_vector_df = pd.DataFrame(session_vectors)\n",
    "# session_vector_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_vector_df.to_csv(interim_path + 'train_session_interaction_vector.csv', header=True)\n",
    "a = np.vstack(session_vectors_np)\n",
    "header = \",\".join(map(str, range(len(unqiue_train_items))))\n",
    "np.savetxt(interim_path + 'train_session_interaction_vector.csv', a, header=header, delimiter=\",\", fmt=\"%d\", comments=\",\")\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "interactions.interaction_type.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Num items: 26722\n",
      "Train Num sessions: 51698\n",
      "Train Num events: 125302\n",
      "Test Num items: 4401\n",
      "Test Num sessions: 19711\n",
      "Test Num events: 43122\n"
     ]
    }
   ],
   "source": [
    "print('Train Num items: {}'.format(train_full_sessions.item_id.nunique()))\n",
    "print('Train Num sessions: {}'.format(train_full_sessions.session_id.nunique()))\n",
    "print('Train Num events: {}'.format(len(train_full_sessions)))\n",
    "print('Test Num items: {}'.format(test_sessions.item_id.nunique()))\n",
    "print('Test Num sessions: {}'.format(test_sessions.session_id.nunique()))\n",
    "print('Test Num events: {}'.format(len(test_sessions)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building sessions\n",
      "        user_id  item_id  interaction_type  created_at  session_id\n",
      "25654        13   118310                 1  1484211749           1\n",
      "630641       13  1875610                 1  1486027147           2\n",
      "630640       13  1875610                 2  1486042146           3\n",
      "Original data:\n",
      "Num items: 51181\n",
      "Num users: 249987\n",
      "Num sessions: 456661\n",
      "Filtering data\n",
      "Filtered data:\n",
      "Num items: 23323\n",
      "Num users: 21362\n",
      "Num sessions: 28753\n",
      "Partitioning data\n",
      "Before item count: 8141\n",
      "After item count: 3048\n",
      "Before item count: 8990\n",
      "After item count: 2316\n",
      "Data - Sessions: 28753\n",
      "Training - Sessions: 20075\n",
      "Testing - Sessions: 5257\n",
      "Train Validating - Sessions: 11994\n",
      "Test Validating - Sessions: 3860\n",
      "Train Num items: 18230\n",
      "Train Num sessions: 20075\n",
      "Train Num events: 78666\n",
      "Test Num items: 2755\n",
      "Test Num sessions: 5257\n",
      "Test Num events: 14817\n"
     ]
    }
   ],
   "source": [
    "interactions = pd.read_csv(\"../../data/recsys17/raw/interactions.csv\", header=0, sep='\\t')\n",
    "\n",
    "# remove NaN values (should have only 1)\n",
    "interactions = interactions[np.isfinite(interactions['created_at'])]\n",
    "# convert back to long from float\n",
    "interactions['created_at'] = interactions['created_at'].astype(np.int64)\n",
    "\n",
    "# remove impressions\n",
    "interactions = interactions[interactions.interaction_type >= 1].copy()\n",
    "# remove delete and headhunter event types\n",
    "interactions = interactions[interactions.interaction_type < 4].copy()\n",
    "\n",
    "\n",
    "interactions['interaction_type'] = interactions['interaction_type'].fillna(0).astype('int')\n",
    "\n",
    "\n",
    "\n",
    "print('Building sessions')\n",
    "# partition interactions into sessions with 30-minutes idle time\n",
    "interactions = make_sessions(interactions, session_th=30 * 60, time_key='created_at', is_ordered=False)\n",
    "\n",
    "\n",
    "print(interactions.head(3))\n",
    "# drop 189 duplicate interactions\n",
    "interactions = interactions.drop_duplicates(['item_id','session_id','created_at'])\n",
    "\n",
    "print('Original data:')\n",
    "print('Num items: {}'.format(interactions.item_id.nunique()))\n",
    "print('Num users: {}'.format(interactions.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(interactions.session_id.nunique()))\n",
    "\n",
    "print('Filtering data')\n",
    "# keep items with >=20 interactions\n",
    "item_pop = interactions.item_id.value_counts()\n",
    "good_items = item_pop[item_pop >= 1].index\n",
    "inter_dense = interactions[interactions.item_id.isin(good_items)]\n",
    "# remove sessions with length < 3\n",
    "session_length = inter_dense.session_id.value_counts()\n",
    "good_sessions = session_length[session_length >= 3].index\n",
    "inter_dense = inter_dense[inter_dense.session_id.isin(good_sessions)]\n",
    "# let's keep only returning users (with >= 5 sessions) and remove overly active ones (>=200 sessions)\n",
    "sess_per_user = inter_dense.groupby('user_id')['session_id'].nunique()\n",
    "good_users = sess_per_user[(sess_per_user >= 1) & (sess_per_user < 200000)].index\n",
    "inter_dense = inter_dense[inter_dense.user_id.isin(good_users)]\n",
    "print('Filtered data:')\n",
    "print('Num items: {}'.format(inter_dense.item_id.nunique()))\n",
    "print('Num users: {}'.format(inter_dense.user_id.nunique()))\n",
    "print('Num sessions: {}'.format(inter_dense.session_id.nunique()))\n",
    "\n",
    "store_path = \"../../data/recsys17/\"\n",
    "inter_dense.to_csv(store_path + \"filtered.csv\", sep='\\t')\n",
    "\n",
    "print('Partitioning data')\n",
    "# last-session-out partitioning\n",
    "train_full_sessions, test_sessions = last_n_days_out_split(inter_dense, n=14,\n",
    "                                                            user_key='user_id',\n",
    "                                                            item_key='item_id',\n",
    "                                                            session_key='session_id',\n",
    "                                                            time_key='created_at',\n",
    "                                                            clean_test=True)\n",
    "train_valid_sessions, valid_sessions = last_n_days_out_split(train_full_sessions, n=14,\n",
    "                                                              user_key='user_id',\n",
    "                                                              item_key='item_id',\n",
    "                                                              session_key='session_id',\n",
    "                                                              time_key='created_at',\n",
    "                                                              clean_test=True)\n",
    "\n",
    "print(\"Data - Sessions: \" + str(len(inter_dense.session_id.unique())))\n",
    "print(\"Training - Sessions: \" + str(len(train_full_sessions.session_id.unique())))\n",
    "print(\"Testing - Sessions: \" + str(len(test_sessions.session_id.unique())))\n",
    "print(\"Train Validating - Sessions: \" + str(len(train_valid_sessions.session_id.unique())))\n",
    "print(\"Test Validating - Sessions: \" + str(len(valid_sessions.session_id.unique())))\n",
    "\n",
    "train_full_sessions.to_csv(store_path + \"train_d14.csv\", sep='\\t')\n",
    "test_sessions.to_csv(store_path + \"test_d14.csv\", sep='\\t')\n",
    "train_valid_sessions.to_csv(store_path + \"valid_train_d14.csv\", sep='\\t')\n",
    "valid_sessions.to_csv(store_path + \"valid_test_d14.csv\", sep='\\t')\n",
    "\n",
    "print('Train Num items: {}'.format(train_full_sessions.item_id.nunique()))\n",
    "print('Train Num sessions: {}'.format(train_full_sessions.session_id.nunique()))\n",
    "print('Train Num events: {}'.format(len(train_full_sessions)))\n",
    "print('Test Num items: {}'.format(test_sessions.item_id.nunique()))\n",
    "print('Test Num sessions: {}'.format(test_sessions.session_id.nunique()))\n",
    "print('Test Num events: {}'.format(len(test_sessions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
