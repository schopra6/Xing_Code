{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "from collections import OrderedDict as od\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_file = '../items.csv'\n",
    "users_file = '../users.csv' \n",
    "trimmed_data = True\n",
    "#directory where data will be stored\n",
    "folder = '../prepare_data/test_dataset/'\n",
    "test_interaction_file = folder + 'test_interaction.csv'\n",
    "test_users_file = folder +'test_users.csv'\n",
    "train_users_file = folder + 'train_users.csv'\n",
    "#assigned gender file\n",
    "assigned_users_file = folder + 'assigned_users.csv'\n",
    "unbiased_test_interactions_file = folder + 'unbiased_test_interactions.csv'\n",
    "biased_test_interactions_file = folder + 'biased_test_interactions.csv'\n",
    "train_interactions_file = folder + 'train_interactions.csv'\n",
    "train_negative_sampling_interactions_file = folder + 'negative_interactions.csv'\n",
    "impression_file = '../impressions.csv'\n",
    "jaccard_threshold = 0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_features import *\n",
    "from process_xgb_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_key='user_id'\n",
    "item_key='item_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median,mean,mode\n",
    "def load_file():\n",
    "    items = pd.read_csv(items_file,sep='\\t')\n",
    "    items.fillna(-100,inplace=True)\n",
    "    items = items.rename(columns={\"id\": \"item_id\"}, errors=\"raise\")\n",
    "    test_interaction = pd.read_csv(test_interaction_file)\n",
    "    train_interactions = pd.read_csv(train_interactions_file)\n",
    "    assigned_users = pd.read_csv(assigned_users_file)\n",
    "    assigned_users.fillna(100,inplace=True)\n",
    "    assigned_users=assigned_users.drop_duplicates(subset = [user_key])\n",
    "    items=items.drop_duplicates(subset = [item_key])\n",
    "    assigned_users = assigned_users.rename(columns={\"id\": \"user_id\"})\n",
    "    return items,test_interaction,train_interactions,assigned_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "items,test_interactions,train_interactions,assigned_users = load_file()\n",
    "items = item_process(items)\n",
    "assigned_users = user_process(assigned_users)\n",
    "negative_interactions = negative_sampling(train_interactions,user_key='user_id',item_key='item_id',timestamp=train_interactions.created_at.iloc[0])\n",
    "negative_interactions = negative_interactions.drop('index',axis=1)\n",
    "train_interactions = pd.concat([train_interactions,negative_interactions])\n",
    "train_interactions['created_at'] = pd.to_datetime(train_interactions['created_at'], unit='s')\n",
    "train_interactions['week'] = train_interactions.created_at.dt.strftime('%W')\n",
    "positive_train_interactions = train_interactions[train_interactions['interaction_type']!=4]\n",
    "#item_clicks = clicks(positive_train_interactions)\n",
    "#click_ratio_df = click_ratio(positive_train_interactions)\n",
    "#user_events = user_activity(train_interactions)\n",
    "#item_id_max,user_id_max = click_date_max(train_interactions)\n",
    "train_item_ids = train_interactions['item_id'].unique()\n",
    "test_item_ids = test_interactions['item_id'].unique()\n",
    "train_user_ids = train_interactions['user_id'].unique()\n",
    "test_user_ids = test_interactions['user_id'].unique()\n",
    "item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "items = items[items['item_id'].isin(item_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "intu,intuser = Intu_Intusers(positive_train_interactions,assigned_users.set_index(user_key),items.set_index(item_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>intu_career_level</th>\n",
       "      <th>intu_discipline_id</th>\n",
       "      <th>intu_industry_id</th>\n",
       "      <th>intu_region</th>\n",
       "      <th>intu_country</th>\n",
       "      <th>intu_latitude</th>\n",
       "      <th>intu_longitude</th>\n",
       "      <th>intu_mode_career_level</th>\n",
       "      <th>intu_mode_industry_id</th>\n",
       "      <th>intu_mode_discipline_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>83</td>\n",
       "      <td>[470674, 550818, 649130, 1893735, 847916, 1510...</td>\n",
       "      <td>[3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, ...</td>\n",
       "      <td>[20, 16, 15, 15, 15, 8, 13, 15, 5, 13, 15, 16,...</td>\n",
       "      <td>[15, 9, 11, 11, 11, 15, 11, 11, 21, 11, 11, 9,...</td>\n",
       "      <td>[0, 1, 9, 0, 2, 1, 0, 0, 10, 0, 0, 1, 0, 7, 6,...</td>\n",
       "      <td>[de, de, de, non_dach, de, de, ch, ch, de, ch,...</td>\n",
       "      <td>[-100.0, 48.8, 52.3, 30.7, 50.3, 49.8, 47.5, 4...</td>\n",
       "      <td>[-100.0, 9.2, 10.2, -88.0, 8.9, 10.0, 8.7, 7.4...</td>\n",
       "      <td>[1.0, 3.0]</td>\n",
       "      <td>[15, 11]</td>\n",
       "      <td>[16, 15]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            item_id  \\\n",
       "0       83  [470674, 550818, 649130, 1893735, 847916, 1510...   \n",
       "\n",
       "                                   intu_career_level  \\\n",
       "0  [3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, 3.0, 2.0, ...   \n",
       "\n",
       "                                  intu_discipline_id  \\\n",
       "0  [20, 16, 15, 15, 15, 8, 13, 15, 5, 13, 15, 16,...   \n",
       "\n",
       "                                    intu_industry_id  \\\n",
       "0  [15, 9, 11, 11, 11, 15, 11, 11, 21, 11, 11, 9,...   \n",
       "\n",
       "                                         intu_region  \\\n",
       "0  [0, 1, 9, 0, 2, 1, 0, 0, 10, 0, 0, 1, 0, 7, 6,...   \n",
       "\n",
       "                                        intu_country  \\\n",
       "0  [de, de, de, non_dach, de, de, ch, ch, de, ch,...   \n",
       "\n",
       "                                       intu_latitude  \\\n",
       "0  [-100.0, 48.8, 52.3, 30.7, 50.3, 49.8, 47.5, 4...   \n",
       "\n",
       "                                      intu_longitude intu_mode_career_level  \\\n",
       "0  [-100.0, 9.2, 10.2, -88.0, 8.9, 10.0, 8.7, 7.4...             [1.0, 3.0]   \n",
       "\n",
       "  intu_mode_industry_id intu_mode_discipline_id  \n",
       "0              [15, 11]                [16, 15]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intu[intu.set_index('user_id').index.isin([83])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = item_postprocess(items,[intuser])\n",
    "assigned_users = user_postprocess(assigned_users,[intu])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =train_data.drop('item_id_y',axis=1).rename(columns={'item_id_x':'item_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_interactions.merge(assigned_users,left_on=user_key,right_on=user_key)\n",
    "train_data =train_data.merge(items,left_on=item_key,right_on=item_key,suffixes=('_user', '_item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data =train_data.merge(items,left_on=item_key,right_on=item_key,suffixes=('_user', '_item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [3.0, 1.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...\n",
      "1        [3.0, 1.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...\n",
      "2        [3.0, 1.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...\n",
      "3        [3.0, 3.0, 1.0, 1.0, 3.0, 3.0, 3.0, 3.0, 1.0, ...\n",
      "4        [3.0, 1.0, 3.0, 1.0, 3.0, 3.0, 3.0, 3.0, 3.0, ...\n",
      "                               ...                        \n",
      "69932    [1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 3.0, ...\n",
      "69933    [1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 3.0, ...\n",
      "69934    [1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 3.0, ...\n",
      "69935    [1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 3.0, ...\n",
      "69936    [1.0, 3.0, 2.0, 3.0, 3.0, 1.0, 3.0, 3.0, 3.0, ...\n",
      "Name: intu_career_level, Length: 69937, dtype: object\n",
      "0        [9, 1, 2, 1, 9, 0, 9, 0, 9, 9, 3, 0, 4, 9, 9, ...\n",
      "1        [9, 1, 2, 1, 9, 0, 9, 0, 9, 9, 3, 0, 4, 9, 9, ...\n",
      "2        [9, 1, 2, 1, 9, 0, 9, 0, 9, 9, 3, 0, 4, 9, 9, ...\n",
      "3        [0, 9, 0, 1, 0, 4, 1, 1, 9, 0, 1, 9, 9, 0, 9, ...\n",
      "4        [9, 1, 2, 1, 9, 0, 9, 0, 9, 9, 3, 0, 4, 9, 9, ...\n",
      "                               ...                        \n",
      "69932       [8, 1, 2, 0, 5, 10, 0, 2, 1, 2, 0, 0, 2, 0, 0]\n",
      "69933       [8, 1, 2, 0, 5, 10, 0, 2, 1, 2, 0, 0, 2, 0, 0]\n",
      "69934       [8, 1, 2, 0, 5, 10, 0, 2, 1, 2, 0, 0, 2, 0, 0]\n",
      "69935       [8, 1, 2, 0, 5, 10, 0, 2, 1, 2, 0, 0, 2, 0, 0]\n",
      "69936       [8, 1, 2, 0, 5, 10, 0, 2, 1, 2, 0, 0, 2, 0, 0]\n",
      "Name: intu_region, Length: 69937, dtype: object\n",
      "0        [21, 14, 11, 10, 15, 15, 21, 15, 21, 15, 18, 8...\n",
      "1        [21, 14, 11, 10, 15, 15, 21, 15, 21, 15, 18, 8...\n",
      "2        [21, 14, 11, 10, 15, 15, 21, 15, 21, 15, 18, 8...\n",
      "3        [15, 15, 8, 14, 15, 11, 15, 15, 1, 15, 15, 15,...\n",
      "4        [21, 14, 11, 10, 15, 15, 21, 15, 21, 15, 18, 8...\n",
      "                               ...                        \n",
      "69932    [15, 21, 18, 11, 11, 8, 21, 11, 21, 4, 21, 11,...\n",
      "69933    [15, 21, 18, 11, 11, 8, 21, 11, 21, 4, 21, 11,...\n",
      "69934    [15, 21, 18, 11, 11, 8, 21, 11, 21, 4, 21, 11,...\n",
      "69935    [15, 21, 18, 11, 11, 8, 21, 11, 21, 4, 21, 11,...\n",
      "69936    [15, 21, 18, 11, 11, 8, 21, 11, 21, 4, 21, 11,...\n",
      "Name: intu_industry_id, Length: 69937, dtype: object\n",
      "0        [23, 14, 15, 5, 15, 14, 23, 14, 23, 12, 9, 14,...\n",
      "1        [23, 14, 15, 5, 15, 14, 23, 14, 23, 12, 9, 14,...\n",
      "2        [23, 14, 15, 5, 15, 14, 23, 14, 23, 12, 9, 14,...\n",
      "3        [16, 15, 16, 14, 16, 16, 5, 5, 16, 12, 5, 14, ...\n",
      "4        [23, 14, 15, 5, 15, 14, 23, 14, 23, 12, 9, 14,...\n",
      "                               ...                        \n",
      "69932    [18, 19, 9, 20, 15, 15, 13, 6, 19, 16, 5, 12, ...\n",
      "69933    [18, 19, 9, 20, 15, 15, 13, 6, 19, 16, 5, 12, ...\n",
      "69934    [18, 19, 9, 20, 15, 15, 13, 6, 19, 16, 5, 12, ...\n",
      "69935    [18, 19, 9, 20, 15, 15, 13, 6, 19, 16, 5, 12, ...\n",
      "69936    [18, 19, 9, 20, 15, 15, 13, 6, 19, 16, 5, 12, ...\n",
      "Name: intu_discipline_id, Length: 69937, dtype: object\n",
      "0        [de, de, de, de, de, de, de, de, de, de, de, c...\n",
      "1        [de, de, de, de, de, de, de, de, de, de, de, c...\n",
      "2        [de, de, de, de, de, de, de, de, de, de, de, c...\n",
      "3        [ch, de, de, de, ch, de, de, de, de, ch, de, d...\n",
      "4        [de, de, de, de, de, de, de, de, de, de, de, c...\n",
      "                               ...                        \n",
      "69932    [de, de, de, ch, de, de, ch, de, de, de, non_d...\n",
      "69933    [de, de, de, ch, de, de, ch, de, de, de, non_d...\n",
      "69934    [de, de, de, ch, de, de, ch, de, de, de, non_d...\n",
      "69935    [de, de, de, ch, de, de, ch, de, de, de, non_d...\n",
      "69936    [de, de, de, ch, de, de, ch, de, de, de, non_d...\n",
      "Name: intu_country, Length: 69937, dtype: object\n",
      "0        [3, 3, 3, 100]\n",
      "1        [3, 3, 3, 100]\n",
      "2        [3, 3, 3, 100]\n",
      "3        [3, 3, 3, 100]\n",
      "4              [3, 100]\n",
      "              ...      \n",
      "69932               [0]\n",
      "69933               [0]\n",
      "69934               [0]\n",
      "69935               [0]\n",
      "69936               [0]\n",
      "Name: intuser_career_level, Length: 69937, dtype: object\n",
      "0        [1.0, 1.0, 1.0, 1.0]\n",
      "1        [1.0, 1.0, 1.0, 1.0]\n",
      "2        [1.0, 1.0, 1.0, 1.0]\n",
      "3        [1.0, 1.0, 1.0, 1.0]\n",
      "4                  [1.0, 1.0]\n",
      "                 ...         \n",
      "69932                   [0.0]\n",
      "69933                   [0.0]\n",
      "69934                   [0.0]\n",
      "69935                   [0.0]\n",
      "69936                   [0.0]\n",
      "Name: intuser_region, Length: 69937, dtype: object\n",
      "0        [1, 1, 1, 0]\n",
      "1        [1, 1, 1, 0]\n",
      "2        [1, 1, 1, 0]\n",
      "3        [1, 1, 1, 0]\n",
      "4              [1, 0]\n",
      "             ...     \n",
      "69932             [0]\n",
      "69933             [0]\n",
      "69934             [0]\n",
      "69935             [0]\n",
      "69936             [0]\n",
      "Name: intuser_industry_id, Length: 69937, dtype: object\n",
      "0        [20, 20, 20, 0]\n",
      "1        [20, 20, 20, 0]\n",
      "2        [20, 20, 20, 0]\n",
      "3        [20, 20, 20, 0]\n",
      "4                [20, 0]\n",
      "              ...       \n",
      "69932                [0]\n",
      "69933                [0]\n",
      "69934                [0]\n",
      "69935                [0]\n",
      "69936                [0]\n",
      "Name: intuser_discipline_id, Length: 69937, dtype: object\n",
      "0        [de, de, de, de]\n",
      "1        [de, de, de, de]\n",
      "2        [de, de, de, de]\n",
      "3        [de, de, de, de]\n",
      "4                [de, de]\n",
      "               ...       \n",
      "69932                [de]\n",
      "69933                [de]\n",
      "69934                [de]\n",
      "69935                [de]\n",
      "69936                [de]\n",
      "Name: intuser_country, Length: 69937, dtype: object\n",
      "0        [6, 6, 6, 4]\n",
      "1        [6, 6, 6, 4]\n",
      "2        [6, 6, 6, 4]\n",
      "3        [6, 6, 6, 4]\n",
      "4              [6, 4]\n",
      "             ...     \n",
      "69932             [3]\n",
      "69933             [3]\n",
      "69934             [3]\n",
      "69935             [3]\n",
      "69936             [3]\n",
      "Name: intuser_experience_years_experience, Length: 69937, dtype: object\n",
      "0        [2, 2, 2, 2]\n",
      "1        [2, 2, 2, 2]\n",
      "2        [2, 2, 2, 2]\n",
      "3        [2, 2, 2, 2]\n",
      "4              [2, 2]\n",
      "             ...     \n",
      "69932             [2]\n",
      "69933             [2]\n",
      "69934             [2]\n",
      "69935             [2]\n",
      "69936             [2]\n",
      "Name: intuser_edu_degree, Length: 69937, dtype: object\n",
      "0        [3, 3, 3, 2]\n",
      "1        [3, 3, 3, 2]\n",
      "2        [3, 3, 3, 2]\n",
      "3        [3, 3, 3, 2]\n",
      "4              [3, 2]\n",
      "             ...     \n",
      "69932             [1]\n",
      "69933             [1]\n",
      "69934             [1]\n",
      "69935             [1]\n",
      "69936             [1]\n",
      "Name: intuser_experience_years_in_current, Length: 69937, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>interaction_type</th>\n",
       "      <th>created_at_user</th>\n",
       "      <th>week</th>\n",
       "      <th>jobroles</th>\n",
       "      <th>career_level_user</th>\n",
       "      <th>discipline_id_user</th>\n",
       "      <th>industry_id_user</th>\n",
       "      <th>...</th>\n",
       "      <th>intu_discipline_id_item_contained</th>\n",
       "      <th>intu_country_item_contained</th>\n",
       "      <th>intuser_career_level_user_contained</th>\n",
       "      <th>intuser_region_user_contained</th>\n",
       "      <th>intuser_industry_id_user_contained</th>\n",
       "      <th>intuser_discipline_id_user_contained</th>\n",
       "      <th>intuser_country_user_contained</th>\n",
       "      <th>intuser_experience_years_experience_contained</th>\n",
       "      <th>intuser_edu_degree_contained</th>\n",
       "      <th>intuser_experience_years_in_current_contained</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3791.0</td>\n",
       "      <td>865830</td>\n",
       "      <td>1427054</td>\n",
       "      <td>3</td>\n",
       "      <td>2015-10-30 06:29:04</td>\n",
       "      <td>43</td>\n",
       "      <td>1072229</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3801.0</td>\n",
       "      <td>865830</td>\n",
       "      <td>1427054</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-31 16:52:26</td>\n",
       "      <td>43</td>\n",
       "      <td>1072229</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3805.0</td>\n",
       "      <td>865830</td>\n",
       "      <td>1427054</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-30 06:28:17</td>\n",
       "      <td>43</td>\n",
       "      <td>1072229</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5140281.0</td>\n",
       "      <td>2855502</td>\n",
       "      <td>1427054</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-21 06:34:44</td>\n",
       "      <td>42</td>\n",
       "      <td>3731273</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3792.0</td>\n",
       "      <td>865830</td>\n",
       "      <td>2312253</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-11-03 14:14:03</td>\n",
       "      <td>44</td>\n",
       "      <td>1072229</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.882</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69932</th>\n",
       "      <td>8826621.0</td>\n",
       "      <td>2798291</td>\n",
       "      <td>1444038</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-23 06:24:15</td>\n",
       "      <td>42</td>\n",
       "      <td>3580664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69933</th>\n",
       "      <td>8826616.0</td>\n",
       "      <td>2798291</td>\n",
       "      <td>1508101</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-18 19:10:47</td>\n",
       "      <td>41</td>\n",
       "      <td>3580664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69934</th>\n",
       "      <td>8826619.0</td>\n",
       "      <td>2798291</td>\n",
       "      <td>1301190</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-18 19:08:39</td>\n",
       "      <td>41</td>\n",
       "      <td>3580664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69935</th>\n",
       "      <td>8826623.0</td>\n",
       "      <td>2798291</td>\n",
       "      <td>1649684</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-18 19:08:15</td>\n",
       "      <td>41</td>\n",
       "      <td>3580664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.133</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69936</th>\n",
       "      <td>8826617.0</td>\n",
       "      <td>2798291</td>\n",
       "      <td>901493</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-12 20:56:42</td>\n",
       "      <td>41</td>\n",
       "      <td>3580664</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.200</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69937 rows Ã— 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  user_id  item_id  interaction_type     created_at_user  \\\n",
       "0          3791.0   865830  1427054                 3 2015-10-30 06:29:04   \n",
       "1          3801.0   865830  1427054                 1 2015-10-31 16:52:26   \n",
       "2          3805.0   865830  1427054                 1 2015-10-30 06:28:17   \n",
       "3       5140281.0  2855502  1427054                 1 2015-10-21 06:34:44   \n",
       "4          3792.0   865830  2312253                 1 2015-11-03 14:14:03   \n",
       "...           ...      ...      ...               ...                 ...   \n",
       "69932   8826621.0  2798291  1444038                 1 2015-10-23 06:24:15   \n",
       "69933   8826616.0  2798291  1508101                 1 2015-10-18 19:10:47   \n",
       "69934   8826619.0  2798291  1301190                 1 2015-10-18 19:08:39   \n",
       "69935   8826623.0  2798291  1649684                 1 2015-10-18 19:08:15   \n",
       "69936   8826617.0  2798291   901493                 1 2015-10-12 20:56:42   \n",
       "\n",
       "      week jobroles  career_level_user  discipline_id_user  industry_id_user  \\\n",
       "0       43  1072229                  3                  20                 1   \n",
       "1       43  1072229                  3                  20                 1   \n",
       "2       43  1072229                  3                  20                 1   \n",
       "3       42  3731273                100                   0                 0   \n",
       "4       44  1072229                  3                  20                 1   \n",
       "...    ...      ...                ...                 ...               ...   \n",
       "69932   42  3580664                  0                   0                 0   \n",
       "69933   41  3580664                  0                   0                 0   \n",
       "69934   41  3580664                  0                   0                 0   \n",
       "69935   41  3580664                  0                   0                 0   \n",
       "69936   41  3580664                  0                   0                 0   \n",
       "\n",
       "       ... intu_discipline_id_item_contained  intu_country_item_contained  \\\n",
       "0      ...                             0.176                        0.882   \n",
       "1      ...                             0.176                        0.882   \n",
       "2      ...                             0.176                        0.882   \n",
       "3      ...                             0.050                        0.650   \n",
       "4      ...                             0.294                        0.882   \n",
       "...    ...                               ...                          ...   \n",
       "69932  ...                             0.067                        0.133   \n",
       "69933  ...                             0.067                        0.067   \n",
       "69934  ...                             0.200                        0.600   \n",
       "69935  ...                             0.200                        0.133   \n",
       "69936  ...                             0.133                        0.200   \n",
       "\n",
       "       intuser_career_level_user_contained  intuser_region_user_contained  \\\n",
       "0                                     0.75                            1.0   \n",
       "1                                     0.75                            1.0   \n",
       "2                                     0.75                            1.0   \n",
       "3                                     0.25                            1.0   \n",
       "4                                     0.50                            1.0   \n",
       "...                                    ...                            ...   \n",
       "69932                                 1.00                            1.0   \n",
       "69933                                 1.00                            1.0   \n",
       "69934                                 1.00                            1.0   \n",
       "69935                                 1.00                            1.0   \n",
       "69936                                 1.00                            1.0   \n",
       "\n",
       "       intuser_industry_id_user_contained  \\\n",
       "0                                    0.75   \n",
       "1                                    0.75   \n",
       "2                                    0.75   \n",
       "3                                    0.25   \n",
       "4                                    0.50   \n",
       "...                                   ...   \n",
       "69932                                1.00   \n",
       "69933                                1.00   \n",
       "69934                                1.00   \n",
       "69935                                1.00   \n",
       "69936                                1.00   \n",
       "\n",
       "       intuser_discipline_id_user_contained intuser_country_user_contained  \\\n",
       "0                                      0.75                            1.0   \n",
       "1                                      0.75                            1.0   \n",
       "2                                      0.75                            1.0   \n",
       "3                                      0.25                            1.0   \n",
       "4                                      0.50                            1.0   \n",
       "...                                     ...                            ...   \n",
       "69932                                  1.00                            1.0   \n",
       "69933                                  1.00                            1.0   \n",
       "69934                                  1.00                            1.0   \n",
       "69935                                  1.00                            1.0   \n",
       "69936                                  1.00                            1.0   \n",
       "\n",
       "       intuser_experience_years_experience_contained  \\\n",
       "0                                               0.75   \n",
       "1                                               0.75   \n",
       "2                                               0.75   \n",
       "3                                               0.25   \n",
       "4                                               0.50   \n",
       "...                                              ...   \n",
       "69932                                           1.00   \n",
       "69933                                           1.00   \n",
       "69934                                           1.00   \n",
       "69935                                           1.00   \n",
       "69936                                           1.00   \n",
       "\n",
       "      intuser_edu_degree_contained  \\\n",
       "0                              1.0   \n",
       "1                              1.0   \n",
       "2                              1.0   \n",
       "3                              1.0   \n",
       "4                              1.0   \n",
       "...                            ...   \n",
       "69932                          1.0   \n",
       "69933                          1.0   \n",
       "69934                          1.0   \n",
       "69935                          1.0   \n",
       "69936                          1.0   \n",
       "\n",
       "      intuser_experience_years_in_current_contained  \n",
       "0                                              0.75  \n",
       "1                                              0.75  \n",
       "2                                              0.75  \n",
       "3                                              0.25  \n",
       "4                                              0.50  \n",
       "...                                             ...  \n",
       "69932                                          1.00  \n",
       "69933                                          1.00  \n",
       "69934                                          1.00  \n",
       "69935                                          1.00  \n",
       "69936                                          1.00  \n",
       "\n",
       "[69937 rows x 64 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_feature(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828972/2900186027.py:204: DeprecationWarning: scipy.sum is deprecated and will be removed in SciPy 2.0.0, use numpy.sum instead\n",
      "  matA_sum = scipy.sum(matA,axis=1)\n",
      "/tmp/ipykernel_3828972/2900186027.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['jobrole_list'] =inter_df['jobrole_list'].apply(set)\n",
      "/tmp/ipykernel_3828972/2900186027.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['title_list'] =inter_df['title_list'].apply(set)\n",
      "/tmp/ipykernel_3828972/2900186027.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['tag_list'] =inter_df['tag_list'].apply(set)\n",
      "/tmp/ipykernel_3828972/2900186027.py:257: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['jobrole_list'] =inter_df['jobrole_list'].apply(set)\n",
      "/tmp/ipykernel_3828972/2900186027.py:258: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['title_list'] =inter_df['title_list'].apply(set)\n",
      "/tmp/ipykernel_3828972/2900186027.py:259: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  inter_df['tag_list'] =inter_df['tag_list'].apply(set)\n"
     ]
    }
   ],
   "source": [
    "items,test_interactions,train_interactions,assigned_users = load_file()\n",
    "items = item_process(items)\n",
    "user_key='user_id'\n",
    "item_key='item_id'\n",
    "assigned_users=assigned_users.drop_duplicates('user_id')\n",
    "assigned_users = user_process(assigned_users)\n",
    "negative_interactions = negative_sampling(train_interactions,user_key='user_id',item_key='item_id',timestamp=train_interactions.created_at.iloc[0])\n",
    "negative_interactions = negative_interactions.drop('index',axis=1)\n",
    "train_interactions = pd.concat([train_interactions,negative_interactions])\n",
    "train_interactions['created_at'] = pd.to_datetime(train_interactions['created_at'], unit='s')\n",
    "train_interactions['week'] = train_interactions.created_at.dt.strftime('%W')\n",
    "positive_train_interactions = train_interactions[train_interactions['interaction_type']!=4]\n",
    "#item_clicks = clicks(positive_train_interactions)\n",
    "#click_ratio_df = click_ratio(positive_train_interactions)\n",
    "#user_events = user_activity(train_interactions)\n",
    "#item_id_max,user_id_max = click_date_max(train_interactions)\n",
    "train_item_ids = train_interactions['item_id'].unique()\n",
    "test_item_ids = test_interactions['item_id'].unique()\n",
    "train_user_ids = train_interactions['user_id'].unique()\n",
    "test_user_ids = test_interactions['user_id'].unique()\n",
    "item_ids = np.unique(np.append(test_item_ids,train_item_ids))\n",
    "items = items[items['item_id'].isin(item_ids)]\n",
    "intu,intuser = Intu_Intusers(positive_train_interactions,assigned_users.set_index('user_id'),items.set_index('item_id'))\n",
    "user_item = intu[['user_id','item_id']].set_index('user_id')\n",
    "intu = intu.drop('item_id',axis=1)\n",
    "\n",
    "items = item_postprocess(items,[intuser,item_clicks,click_ratio_df,item_id_max])\n",
    "assigned_users = user_postprocess(assigned_users,[intu,user_events,user_id_max])\n",
    "\n",
    "\n",
    "\n",
    "train_items = items[items['item_id'].isin(train_item_ids)]\n",
    "test_items = items[items['item_id'].isin(test_item_ids)]\n",
    "user_item = intu_item_similarity(test_user_ids,test_items,intu.set_index('user_id'),user_item)\n",
    "train_users = assigned_users[assigned_users['user_id'].isin(train_user_ids)]\n",
    "test_users = assigned_users[assigned_users['user_id'].isin(test_user_ids)]\n",
    "user_to_row = {user_id: i for i, user_id in enumerate(assigned_users['user_id'].unique())}\n",
    "item_to_col = {item_id: i for i, item_id in enumerate(item_ids)}\n",
    "jobrole_list = sum(assigned_users.jobrole_list, [])\n",
    "tag_list = sum(items.tag_list, [])\n",
    "title_list = sum(items.title_list, [])\n",
    "item_tags = set(jobrole_list + tag_list +title_list)\n",
    "tags_elements_dict =id_to_index(item_tags)\n",
    "user_jobrole = test_users[['user_id','jobrole_list']].explode('jobrole_list')\n",
    "user_jobrole['value'] =1\n",
    "user_jobrole_sparse = sparcify(user_jobrole,'user_id','jobrole_list','value',user_to_row,tags_elements_dict)\n",
    "\n",
    "train_item_title_sparse,train_item_tag_sparse = title_tag_sparse(train_items,item_to_col,tags_elements_dict)\n",
    "test_item_title_sparse,test_item_tag_sparse = title_tag_sparse(test_items,item_to_col,tags_elements_dict)\n",
    "\n",
    "testuser_i =[ user_to_row[id] for id in test_user_ids]\n",
    "testuser_i = np.array(testuser_i)\n",
    "sparse_collaborative_matrix = sparcify(positive_train_interactions.drop_duplicates(subset=['user_id','item_id']),'user_id','item_id','interaction_type',user_to_row,item_to_col)\n",
    "testuser_mat= sparse_collaborative_matrix[testuser_i,:]\n",
    "jaccard = jaccard_similarity(testuser_mat,sparse_collaborative_matrix)\n",
    "user_item =jaccard_util(jaccard,test_user_ids,user_item,jaccard_threshold=0.5)\n",
    "\n",
    "user_item = similarity_util(sparse_collaborative_matrix,test_user_ids,train_item_title_sparse,test_item_title_sparse,train_item_tag_sparse,test_item_tag_sparse,user_jobrole_sparse,user_item)\n",
    "user_item = intu_item_similarity(test_user_ids,test_items,intu.set_index('user_id'),user_item)\n",
    "test_data = user_item.reset_index().explode(['item_id'])\n",
    "test_data = test_data.drop_duplicates(subset =['user_id','item_id'])\n",
    "train_data,test_data = train_test_data(train_interactions,test_data,assigned_users,items,test_user_ids,test_item_ids,user_key='user_id',item_key='item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median,mean,mode\n",
    "def load_file():\n",
    "    items = pd.read_csv(items_file,sep='\\t')\n",
    "    items.fillna(-100,inplace=True)\n",
    "    items = items.rename(columns={\"id\": \"item_id\"}, errors=\"raise\")\n",
    "    test_interaction = pd.read_csv(test_interaction_file)\n",
    "    train_interactions = pd.read_csv(train_interactions_file)\n",
    "    assigned_users = pd.read_csv(assigned_users_file)\n",
    "    assigned_users.fillna(100,inplace=True)\n",
    "    assigned_users=assigned_users.drop_duplicates(subset = [user_key])\n",
    "    items=items.drop_duplicates(subset = [items_key])\n",
    "    assigned_users = assigned_users.rename(columns={\"id\": \"user_id\"})\n",
    "    return items,test_interaction,train_interactions,assigned_users\n",
    "\n",
    "def missed_item_stats(recommended_list, groundtruth_list,test_user_ids):\n",
    "  left_out_ids=[]  \n",
    "  for user in test_user_ids:\n",
    "    left_out_ids.append(len(set(groundtruth_list.loc[user]['item_id']) - set(groundtruth_list.loc[user]['item_id'])))\n",
    "  print(f\"mean {mean(left_out_ids)} median {median(left_out_ids)} and mode {mode(left_out_ids)}\")\n",
    "\n",
    "import random\n",
    "def negative_sampling(train_interactions,user_key,item_key,sample_size=1,feedback_key='interaction_type',timestamp_key='created_at',timestamp=None):\n",
    "    unique_users = train_interactions[user_key].drop_duplicates()\n",
    "    unique_items = train_interactions[item_key].unique()\n",
    "    non_interacted_pairs=unique_users.map(lambda user:  (user,set(random.sample(unique_items.tolist(),sample_size)) \n",
    "                                                         - set(train_interactions[train_interactions[user_key] == user][item_key])))\n",
    "    non_interacted_df = pd.DataFrame(non_interacted_pairs.to_list(), columns=[user_key,item_key])\n",
    "    non_interacted_df[item_key]= non_interacted_df[item_key].apply(list)\n",
    "    non_interacted_df =non_interacted_df.explode(item_key).reset_index()\n",
    "    non_interacted_df[feedback_key] =4\n",
    "    non_interacted_df[timestamp_key] =timestamp\n",
    "    return non_interacted_df\n",
    "    \n",
    "      \n",
    "def train_test_data(train_data,test_data,users,items,test_user_ids,test_item_ids,user_key='user_id',item_key='item_id'):\n",
    "    test_data = test_data.drop_duplicates(subset =['user_id','item_id'])\n",
    "    test_data = test_data[test_data['user_id'].isin(test_user_ids)]\n",
    "    items = intuser_process(items)\n",
    "    train_data=train_data.merge(users,left_on=user_key,right_on=user_key)\n",
    "    train_data =train_data.merge(items,left_on=item_key,right_on=item_key,suffixes=('_user', '_item'))\n",
    "    test_data=test_data.merge(users,left_on=user_key,right_on=user_key)\n",
    "    test_data=test_data.merge(items,left_on=item_key,right_on=item_key,suffixes=('_user', '_item'))\n",
    "    train_data = content_similarity(train_data)\n",
    "    test_data = content_similarity(test_data)\n",
    "    train_data = last_click_activity(train_data)\n",
    "    test_data = last_click_activity(test_data)\n",
    "    train_data = event_feature(train_data)\n",
    "    test_data = event_feature(test_data)\n",
    "    return train_data,test_data\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def sparcify(df,row_name,col_name,value_name,user_to_row,item_to_col,row_shape=None,col_shape=None,):\n",
    "\n",
    "    # Create a sparse matrix directly from data\n",
    "    if not row_shape:\n",
    "        row_shape = len(user_to_row)\n",
    "    if not col_shape:    \n",
    "        col_shape = len(item_to_col)\n",
    "\n",
    "    # Create dictionaries to map user/item IDs to matrix indices\n",
    "    #user_to_row = {user_id: i for i, user_id in enumerate(df[row_name].unique())}\n",
    "    #item_to_col = {item_id: i for i, item_id in enumerate(df[col_name].unique())}\n",
    "    # Initialize arrays to store rows, columns, and values for the sparse matrix\n",
    "    rows = np.array([user_to_row[user_id] for user_id in df[row_name]])\n",
    "    cols = np.array([item_to_col[item_id] for item_id in df[col_name]])\n",
    "    values = np.repeat(1,len(rows))\n",
    "    #df[value_name].values\n",
    "    # Create the sparse matrix\n",
    "    sparse_collaborative_matrix = csr_matrix((values, (rows, cols)), shape=(row_shape, col_shape))\n",
    "    return sparse_collaborative_matrix\n",
    "\n",
    "    \n",
    "def id_to_index(df):\n",
    "    item_to_col = {item_id: i for i, item_id in enumerate(df)}\n",
    "    return item_to_col\n",
    "\n",
    "def intuser_process(items):\n",
    "    items['intuser_career_level'] = items['intuser_career_level'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_discipline_id'] = items['intuser_discipline_id'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_industry_id'] = items['intuser_industry_id'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_region'] = items['intuser_region'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_country'] = items['intuser_country'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_experience_years_experience'] = items['intuser_experience_years_experience'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_experience_years_in_current'] = items['intuser_experience_years_in_current'].apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    items['intuser_edu_degree'] = items['intuser_edu_degree'].fillna(0).apply(lambda x: [0] if  str(x) == 'nan' else x)\n",
    "    return items\n",
    "\n",
    "def item_postprocess(items,merge_dfs,item_key='item_id'):\n",
    "    for merge_df in merge_dfs:\n",
    "       items=items.merge(merge_df,left_on=item_key,right_on=item_key,how='left')\n",
    "    return(items)   \n",
    "\n",
    "def user_postprocess(users,merge_dfs,user_key='user_id'):\n",
    "    for merge_df in merge_dfs:\n",
    "        users=users.merge(merge_df,left_on=user_key,right_on=user_key,how='left')\n",
    "    return(users)   \n",
    "def event_feature(df):\n",
    "    columns_users = ['career_level_user','region_user','industry_id_user','discipline_id_user','country_user','experience_years_experience','edu_degree','experience_years_in_current']\n",
    "   \n",
    "    columns_item = ['career_level_item','region_item','industry_id_item','discipline_id_item','country_item']\n",
    "    for column in columns_item:\n",
    "        df['intu_'+column+'_contained'] = percentage_contained(df,'intu_'+column.replace('_item',''),column)\n",
    "    for column in columns_users:\n",
    "        df['intuser_'+column+'_contained'] = percentage_contained(df,'intuser_'+column.replace('_user',''),column)\n",
    "    return df    \n",
    "      \n",
    "\n",
    "def clicks(df,user_key='user_id',item_key='item_id'):\n",
    "\n",
    "    item_clicks = df.groupby(item_key)[user_key].count().reset_index(name = 'clicks')\n",
    "    return item_clicks\n",
    "\n",
    "def Intu_Intusers(df,users,items,user_key='user_id',item_key='item_id'):  \n",
    "\n",
    "    Intusers = df.groupby([item_key])[user_key].apply(list).reset_index()\n",
    "    Intu =df.groupby([user_key])[item_key].apply(list).reset_index()\n",
    "    Intucolumns =['career_level', 'discipline_id', 'industry_id','region','country','latitude','longitude']\n",
    "    Intusercolumns = [ 'career_level', 'discipline_id', 'industry_id','region','country',\n",
    "       'experience_years_experience', 'experience_years_in_current','edu_degree']\n",
    "    for column in Intucolumns:\n",
    "           Intu['intu_'+ column] = Intu[item_key].apply(lambda items_list : [items.loc[item][column] for item in items_list])\n",
    "    for column in Intusercolumns:\n",
    "           Intusers['intuser_'+ column] = Intusers[user_key].apply(lambda users_list : [users.loc[user][column] for user in users_list]) \n",
    "    Intu['intu_mode_career_level'] = Intu['intu_career_level'].map(lambda x : sorted(set(x), key=x.count)[-2:])\n",
    "    Intu['intu_mode_industry_id'] = Intu['intu_industry_id'].map(lambda x :  sorted(set(x), key=x.count)[-2:])\n",
    "    Intu['intu_mode_discipline_id'] = Intu['intu_discipline_id'].map(lambda x :  sorted(set(x), key=x.count)[-2:])  \n",
    "    Intusers.drop(user_key,axis=1,inplace=True) \n",
    "    return Intu,Intusers  \n",
    "\n",
    "def item_process(items):\n",
    "    items['title_list'] = items['title'].apply(lambda x: [int(ele ) for ele in str(x).split(\",\")])\n",
    "    items['tag_list'] = items['tags'].apply(lambda x: [int(ele ) for ele in str(x).split(\",\")])\n",
    "    #train_items =positive_items[positive_items['id'].isin(dropped_interactions_train[item_key].unique())]\n",
    "    #test_items =positive_items[positive_items['id'].isin(dropped_interactions_test[item_key].unique())]\n",
    "    #items =items[items[user_key].isin(train_interactions[item_key].unique())] \n",
    "    return items   \n",
    "\n",
    "def intu_item_similarity(test_user_ids,test_items,Intu,user_items):\n",
    "    for row_num,ele in enumerate(test_user_ids):\n",
    "        intu_mode_career_level = Intu.loc[ele]['intu_mode_career_level']\n",
    "        intu_mode_industry_id =  Intu.loc[ele]['intu_mode_industry_id']\n",
    "        intu_mode_discipline_id =  Intu.loc[ele]['intu_mode_discipline_id']\n",
    "        item_id_list = test_items[(test_items['career_level'].isin(intu_mode_career_level)) \n",
    "                                    & (test_items['industry_id'].isin(intu_mode_industry_id)) & \n",
    "                                    (test_items['discipline_id'].isin(intu_mode_discipline_id))]['item_id'].tolist()\n",
    "\n",
    "        user_items['item_id'].loc[ele]=user_items['item_id'].loc[ele] + item_id_list   \n",
    "    return user_items\n",
    "\n",
    "def user_process(assigned_users):\n",
    "     assigned_users['jobrole_list'] = assigned_users['jobroles'].apply(lambda x: [int(ele ) if len(str(x)) > 0 and str(x) != 'nan' else -100 for ele in str(x).split(\",\")])\n",
    "     return assigned_users\n",
    "\n",
    "def user_item_process():\n",
    "        item_to_col = {item_id: i for i, item_id in enumerate(dropped_interactions_train['item_id'].unique())}\n",
    "        user_items =Intu.copy()\n",
    "        sparse_collaborative_matrix = sparcify(train_data,row_name,col_name,value_name,user_to_row,item_to_col)\n",
    "        testuser_i =[ user_to_row[id] for id in test_user_ids]\n",
    "        testuser_i = np.array(testuser_i)\n",
    "        testuser_mat= sparse_collaborative_matrix[testuser_i,:]\n",
    "        jaccard = jaccard_similarity(testuser_mat,sparse_collaborative_matrix)\n",
    "        jaccard_util(jaccard,jaccard_threshold,test_user_ids,user_items)\n",
    "\n",
    "\n",
    "\n",
    "def jaccard_util(jaccard,test_user_ids,user_items,jaccard_threshold):\n",
    "    for row_num,ele in enumerate(test_user_ids):\n",
    "        user_indexes  =np.argwhere(jaccard[row_num]>jaccard_threshold)[:,1]\n",
    "        item_indexes = set(sparse_collaborative_matrix[user_indexes,:].nonzero()[1])\n",
    "        item_id_list = np.take(item_ids,list(item_indexes))\n",
    "        user_items['item_id'].loc[ele] = user_items['item_id'].loc[ele]+list(item_id_list)\n",
    "    return user_items\n",
    "\n",
    "def similarity_util(sparse_collaborative_matrix,test_user_ids,train_item_title_sparse,test_item_title_sparse,train_item_tag_sparse,test_item_tag_sparse,user_jobrole_sparse,user_items):\n",
    "    ################################################\n",
    "    title_title_similarity = intersection_similarity(train_item_title_sparse,test_item_title_sparse)\n",
    "    title_tag_similarity = intersection_similarity(train_item_title_sparse,test_item_tag_sparse)\n",
    "    tag_title_similarity = intersection_similarity(train_item_tag_sparse,test_item_title_sparse)\n",
    "    tag_tag_similarity = intersection_similarity(train_item_tag_sparse,test_item_tag_sparse)\n",
    "    jobrole_title_similarity = intersection_similarity(user_jobrole_sparse,test_item_tag_sparse)\n",
    "    jobrole_tag_similarity = intersection_similarity(user_jobrole_sparse,test_item_title_sparse)\n",
    "    for row_num,ele in enumerate(test_user_ids):\n",
    "        testuser_i =user_to_row[ele]\n",
    "        item_indexes = set(sparse_collaborative_matrix[testuser_i,:].nonzero()[1])\n",
    "        titi_similar_item_indexes = list(chain(*[title_title_similarity[item_index,:].indices.tolist() for item_index in item_indexes]))\n",
    "        tita_similar_item_indexes =list(chain(*[title_tag_similarity[item_index,:].indices[np.argsort(-title_tag_similarity[item_index,:].data)][:5].tolist() for item_index in item_indexes]))\n",
    "        tati_similar_item_indexes =list(chain(*[tag_title_similarity[item_index,:].indices[np.argsort(-tag_title_similarity[item_index,:].data)][:5].tolist() for item_index in item_indexes]))\n",
    "        tata_similar_item_indexes =list(chain(*[tag_tag_similarity[item_index,:].indices[np.argsort(-tag_tag_similarity[item_index,:].data)][:3].tolist() for item_index in item_indexes]))\n",
    "        joti_similar_item_indexes =list(chain(*[jobrole_title_similarity[testuser_i,:].indices.tolist()]))\n",
    "        jota_similar_item_indexes =list(chain(*[jobrole_tag_similarity[testuser_i,:].indices[np.argsort(-jobrole_tag_similarity[testuser_i,:].data)][:5].tolist()]))\n",
    "        item_i = titi_similar_item_indexes + tita_similar_item_indexes + list(set(tati_similar_item_indexes).intersection(tata_similar_item_indexes).intersection(set(jota_similar_item_indexes))) + joti_similar_item_indexes \n",
    "        #item_i=list(chain(*item_i))\n",
    "        item_id_list = np.unique(np.take(item_ids,item_i))\n",
    "        item_id_list = item_id_list[np.in1d(item_id_list, test_item_ids, assume_unique=True)]\n",
    "        user_items['item_id'].loc[ele]=user_items['item_id'].loc[ele]+list(item_id_list)\n",
    "    return user_items\n",
    "\n",
    "    \n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def jaccard_similarity(matA,matB):\n",
    "\n",
    "    intersection = matA.dot(matB.T)\n",
    "    matA_sum = scipy.sum(matA,axis=1)\n",
    "    matB_sum = matB.sum(axis=1)\n",
    "    union = np.array(np.squeeze(matB_sum))[0] +matA_sum -intersection\n",
    "    jaccard = intersection/union\n",
    "    return jaccard\n",
    "\n",
    "def click_ratio(df,user_key='user_id',item_key='item_id'):\n",
    "\n",
    "    df = df.join(df.groupby('item_id')['created_at'].max(), on='item_id', rsuffix='_last_week_start')\n",
    "    df = df.join(df.groupby('item_id')['created_at'].max(), on='item_id', rsuffix='_previous_last_week_start')\n",
    "    df['created_at_last_week_start'] = df['created_at_last_week_start'] - pd.DateOffset(weeks=1)\n",
    "    df['created_at_previous_last_week_start'] = df['created_at_previous_last_week_start']  - pd.DateOffset(weeks=2)\n",
    "\n",
    "    last_week_data = df[(df['created_at'] > df['created_at_last_week_start'])]\n",
    "    week_before_last_data = df[(df['created_at'] > df['created_at_previous_last_week_start']) & (df['created_at'] <= df['created_at_last_week_start'])]\n",
    "    last_week_click_counts = last_week_data['item_id'].value_counts()\n",
    "    week_before_last_click_counts = week_before_last_data['item_id'].value_counts()\n",
    "    df =df.join(last_week_click_counts/week_before_last_click_counts, on='item_id',rsuffix='_click_ratio')\n",
    "    df['item_id_click_ratio'] = df['item_id_click_ratio'].fillna(1)\n",
    "    df = df[['item_id','item_id_click_ratio']].drop_duplicates(subset=['item_id'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def user_activity(df,user_key='user_id',item_key='item_id'):\n",
    "\n",
    "    user_events = df[[user_key,'item_id']].groupby(user_key)['item_id'].count().reset_index(name = 'user_activity')\n",
    "    return user_events\n",
    "\n",
    "def click_date_max(train):\n",
    "\n",
    "    item_id_max = train.groupby(item_key)['created_at'].max().reset_index(name = 'item_id_max_time')\n",
    "    user_id_max = train.groupby(user_key)['created_at'].max().reset_index(name = 'user_id_max_time')\n",
    "    return item_id_max,user_id_max\n",
    "    \n",
    "def last_click_activity(df):\n",
    "    import pandas as pd\n",
    "\n",
    "# Load your interaction dataframe\n",
    "    # interaction_df = pd.read_csv('interaction_data.csv')\n",
    "\n",
    "    # Sort the dataframe by user_id and timestamp\n",
    "   \n",
    "    max_timestamp =  df['item_id_max_time'].max()\n",
    "    df['max_click_time']=max_timestamp\n",
    "    df['item_user_max_time'] = (df['item_id_max_time']-df['user_id_max_time']).dt.components['hours']\n",
    "    df['item_id_max_time'] = (max_timestamp-df['item_id_max_time']).dt.components['hours']\n",
    "    df['user_id_max_time'] = (max_timestamp-df['user_id_max_time']).dt.components['hours']\n",
    "    return df\n",
    "\n",
    "def content_similarity(df):\n",
    "\n",
    "    df['career_diff'] = df['career_level_user'] -df['career_level_item']\n",
    "    inter_df = df[['jobrole_list','title_list','tag_list']]\n",
    "    inter_df['jobrole_list'] =inter_df['jobrole_list'].apply(set)\n",
    "    inter_df['title_list'] =inter_df['title_list'].apply(set)\n",
    "    inter_df['tag_list'] =inter_df['tag_list'].apply(set)\n",
    "    df['job_title'] = inter_df.apply( lambda x: len(x['jobrole_list'].intersection(x['title_list'])),axis=1)\n",
    "    df['job_tag'] = inter_df.apply( lambda x: len(x['jobrole_list'].intersection(x['tag_list'])),axis=1)\n",
    "    return df\n",
    "\n",
    "def intersection_similarity(matA,matB):\n",
    "    intersection = matA.dot(matB.T)\n",
    "    return intersection\n",
    "\n",
    "def title_tag_sparse(df_items,item_to_col,tags_elements_dict,row_shape=None,col_shape=None):\n",
    "\n",
    "    df_item_title = df_items[['item_id','title_list']].explode('title_list')\n",
    "    df_item_title['value'] =1\n",
    "    #user_to_rowtitle,item_to_coltitle =id_to_index(item_title,'id','title_list')\n",
    "    df_item_title_sparse = sparcify(df_item_title,'item_id','title_list','value',item_to_col,tags_elements_dict,row_shape,col_shape)\n",
    "    df_item_tag = train_items[['item_id','tag_list']].explode('tag_list')\n",
    "    df_item_tag['value'] =1\n",
    "    #user_to_rowtag,item_to_coltag =id_to_index(item_title,'id','tag_list')\n",
    "    df_item_tag_sparse = sparcify(df_item_tag,'item_id','tag_list','value',item_to_col,tags_elements_dict,row_shape,col_shape)\n",
    "    return df_item_title_sparse,df_item_tag_sparse\n",
    "\n",
    "def percentage_contained(df,columnA,columnB):\n",
    "\n",
    "\n",
    "  # we can compute these in-place, no need to create new DataFrame columns\n",
    "  intersect = [elem.count(val) for elem, val in zip(df[columnA], df[columnB])]\n",
    "  a_length = [len(elem) for elem in df[columnA]]\n",
    "\n",
    "  return  [round(i/j,3) for i, j in zip(intersect, a_length)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =train_data[[ 'interaction_type', 'experience_n_entries_class',\n",
    "       'experience_years_experience', 'experience_years_in_current',\n",
    "       'edu_degree', 'latitude',\n",
    "       'longitude','gender',\n",
    "       'clicks',\n",
    "       'item_id_click_ratio', 'item_id_max_time', 'career_diff', 'item_user_max_time',\n",
    "       'intu_career_level_item_contained', 'intu_region_item_contained',\n",
    "       'intu_industry_id_item_contained', 'intu_discipline_id_item_contained',\n",
    "       'intu_country_item_contained', 'intuser_career_level_user_contained',\n",
    "       'intuser_region_user_contained', 'intuser_industry_id_user_contained',\n",
    "       'intuser_discipline_id_user_contained',\n",
    "       'intuser_country_user_contained',\n",
    "       'intuser_experience_years_experience_contained',\n",
    "       'intuser_edu_degree_contained',\n",
    "       'intuser_experience_years_in_current_contained']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test=test_data[[ 'experience_n_entries_class',\n",
    "       'experience_years_experience', 'experience_years_in_current',\n",
    "       'edu_degree', 'latitude',\n",
    "       'longitude','gender',\n",
    "       'clicks',\n",
    "       'item_id_click_ratio', 'item_id_max_time', 'career_diff', 'item_user_max_time',\n",
    "       'intu_career_level_item_contained', 'intu_region_item_contained',\n",
    "       'intu_industry_id_item_contained', 'intu_discipline_id_item_contained',\n",
    "       'intu_country_item_contained', 'intuser_career_level_user_contained',\n",
    "       'intuser_region_user_contained', 'intuser_industry_id_user_contained',\n",
    "       'intuser_discipline_id_user_contained',\n",
    "       'intuser_country_user_contained',\n",
    "       'intuser_experience_years_experience_contained',\n",
    "       'intuser_edu_degree_contained',\n",
    "       'intuser_experience_years_in_current_contained']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3828972/2447738504.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['interaction_type'] = ((data['interaction_type']==1) | (data['interaction_type']==2) | (data['interaction_type']==3)).astype(int)\n"
     ]
    }
   ],
   "source": [
    "data['interaction_type'] = ((data['interaction_type']==1) | (data['interaction_type']==2) | (data['interaction_type']==3)).astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('interaction_type',axis=1), data['interaction_type'], test_size=.3)\n",
    "bst = XGBClassifier(n_estimators=100,n_jobs=100, max_depth=6, learning_rate=0.1,gamma=1,min_child_weight=4,gpu_id=6,tree_method='gpu_hist',objective='binary:logistic')\n",
    "bst.fit(X_train, y_train)\n",
    "y_pred = bst.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['y_pred'] = bst.predict_proba(data_test)[:,1]\n",
    "sorted_items = test_data[['user_id','item_id','y_pred']].sort_values(by=['y_pred'], ascending=[False]).groupby('user_id').head(30)\n",
    "recommended_users = sorted_items.groupby('user_id')['item_id'].agg(list).reset_index()\n",
    "Y_labels =test_interactions[['user_id','item_id']].groupby('user_id')['item_id'].agg(list).reset_index().set_index('user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean 0 median 0.0 and mode 0\n"
     ]
    }
   ],
   "source": [
    "missed_item_stats(recommended_users.set_index('user_id'), Y_labels,test_user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  metrics import challenge_score,compute_metrics,totalRelevant,GCE\n",
    "print(compute_metrics(recommended_users.set_index('user_id'),Y_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
