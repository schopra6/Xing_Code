{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'item_id': [1, 2, 3, 4],\n",
    "    'title': [['1','2'], ['2','3'], ['3','2'], ['4','5']]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the vectorizer on the item titles\n",
    "X = vectorizer.fit_transform(df['title'])\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim_matrix = cosine_similarity(X, X)\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df['item_id'], columns=df['item_id'])\n",
    "\n",
    "print(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "tokenized_list = list(chain(*df['title']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_id    1    2    3    4\n",
      "item_id                    \n",
      "1        0.0  0.0  0.0  0.0\n",
      "2        0.0  0.0  0.0  0.0\n",
      "3        0.0  0.0  0.0  0.0\n",
      "4        0.0  0.0  0.0  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/pairwise.py:1875: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "# Sample data (replace this with your actual data)\n",
    "data = {\n",
    "    'item_id': [1, 2, 3, 4],\n",
    "    'tags': [[2, 8, 4], [2, 8, 4], [7, 5, 2], [3, 6, 9]]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert tags to a numpy array\n",
    "tag_vectors = np.array(df['tags'].tolist())\n",
    "\n",
    "# Calculate the cosine similarity matrix\n",
    "cosine_sim_matrix = pairwise_distances(tag_vectors, metric='jaccard')\n",
    "\n",
    "# Convert the similarity matrix to a DataFrame\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim_matrix, index=df['item_id'], columns=df['item_id'])\n",
    "\n",
    "print(cosine_sim_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Sample data: Replace this with your actual data\n",
    "num_items = 1000000\n",
    "num_tags = 50\n",
    "data = np.random.randint(2, size=(num_items, num_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert tags to binary vectors\n",
    "tag_matrix = pd.get_dummies(tag_vectors, columns=range(1, 1001), prefix='tag')\n",
    "\n",
    "# Calculate Jaccard similarity\n",
    "jaccard_similarity_matrix = pairwise_distances(tag_matrix, metric='jaccard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "\n",
    "# Assuming 'df' is your dataframe with descriptions and tag lists\n",
    "# 'tags' is a list of list of integer tags for each item\n",
    "#df['tags'] = tags\n",
    "\n",
    "# Convert tag lists to sets for faster intersection calculation\n",
    "df['tag_sets'] = df['tags'].apply(set)\n",
    "df['title_sets'] = df['title'].apply(set)\n",
    "\n",
    "# Calculate Jaccard similarity between two sets\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    #union_size = len(set1) + len(set2) - intersection_size\n",
    "    return intersection_size #/ union_size if union_size > 0 else 0.0\n",
    "\n",
    "# Function to compute similarities for a range of items\n",
    "def compute_similarities_range(start, end):\n",
    "    tags_tags_similarities = np.zeros((end - start, num_items))\n",
    "    tags_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_tags_similarities = np.zeros((end - start, num_items))\n",
    "    for i in range(start, end):\n",
    "        tags_tags_similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        tags_title_similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['title_sets'][i], x))\n",
    "        title_tags_similarities[i - start] = df['title_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        title_title_similarities[i - start] = df['title_sets'].apply(lambda x: jaccard_similarity(df['title_sets'][i], x))\n",
    "        similarities = tags_tags_similarities,tags_title_similarities,title_tags_similarities, title_title_similarities\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "    return similarities\n",
    "\n",
    "num_items = len(df)\n",
    "\n",
    "# Define the number of parallel processes based on your system's capabilities\n",
    "num_processes = 4\n",
    "\n",
    "# Split the computation into chunks for parallel processing\n",
    "chunk_size = num_items // num_processes\n",
    "chunks = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_processes - 1)]\n",
    "chunks.append(((num_processes - 1) * chunk_size, num_items))\n",
    "\n",
    "# Initialize a pool of processes\n",
    "with Pool(num_processes) as pool:\n",
    "    results = pool.starmap(compute_similarities_range, chunks)\n",
    "\n",
    "# Combine the results from different processes\n",
    "jaccard_similarity_matrix = np.vstack(results)\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['tags', 'tag_sets'], axis=1, inplace=True)\n",
    "\n",
    "# The jaccard_similarity_matrix will contain similarity values\n",
    "# where jaccard_similarity_matrix[i, j] represents similarity between item i and item j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "\n",
    "# Assuming 'df' is your dataframe with descriptions and tag lists\n",
    "# 'tags' is a list of list of integer tags for each item\n",
    "#df['tags'] = tags\n",
    "\n",
    "# Convert tag lists to sets for faster intersection calculation\n",
    "df['tag_sets'] = df['tags'].apply(set)\n",
    "df['title_sets'] = df['title'].apply(set)\n",
    "\n",
    "# Calculate Jaccard similarity between two sets\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    #union_size = len(set1) + len(set2) - intersection_size\n",
    "    return intersection_size #/ union_size if union_size > 0 else 0.0\n",
    "\n",
    "# Function to compute similarities for a range of items\n",
    "def compute_similarities_range(start, end):\n",
    "    jobroles_tags_similarities = np.zeros((end - start, num_items))\n",
    "    jobroles_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_tags_similarities = np.zeros((end - start, num_items))\n",
    "    for i in range(start, end):\n",
    "        tags_tags_similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        tags_title_similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['title_sets'][i], x))\n",
    "        title_tags_similarities[i - start] = df['title_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        title_title_similarities[i - start] = df['title_sets'].apply(lambda x: jaccard_similarity(df['title_sets'][i], x))\n",
    "        similarities = tags_tags_similarities,tags_title_similarities,title_tags_similarities, title_title_similarities\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "    return similarities\n",
    "\n",
    "num_items = len(df)\n",
    "\n",
    "# Define the number of parallel processes based on your system's capabilities\n",
    "num_processes = 4\n",
    "\n",
    "# Split the computation into chunks for parallel processing\n",
    "chunk_size = num_items // num_processes\n",
    "chunks = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_processes - 1)]\n",
    "chunks.append(((num_processes - 1) * chunk_size, num_items))\n",
    "\n",
    "# Initialize a pool of processes\n",
    "with Pool(num_processes) as pool:\n",
    "    results = pool.starmap(compute_similarities_range, chunks)\n",
    "\n",
    "# Combine the results from different processes\n",
    "jaccard_similarity_matrix = np.vstack(results)\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['tags', 'tag_sets'], axis=1, inplace=True)\n",
    "\n",
    "# The jaccard_similarity_matrix will contain similarity values\n",
    "# where jaccard_similarity_matrix[i, j] represents similarity between item i and item j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import jaccard_score\n",
    "from multiprocessing import Pool\n",
    "from scipy.sparse import coo_matrix, vstack\n",
    "\n",
    "# Assuming 'df' is your dataframe with descriptions and tag lists\n",
    "# 'tags' is a list of list of integer tags for each item\n",
    "#df['tags'] = tags\n",
    "\n",
    "# Convert tag lists to sets for faster intersection calculation\n",
    "df['tag_sets'] = df['tags'].apply(set)\n",
    "df['title_sets'] = df['title'].apply(set)\n",
    "\n",
    "# Calculate Jaccard similarity between two sets\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection_size = len(set1.intersection(set2))\n",
    "    #union_size = len(set1) + len(set2) - intersection_size\n",
    "    return intersection_size #/ union_size if union_size > 0 else 0.0\n",
    "\n",
    "# Function to compute similarities for a range of items\n",
    "def compute_similarities_range(start, end):\n",
    "    tags_tags_similarities = np.zeros((end - start, num_items))\n",
    "    tags_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_title_similarities = np.zeros((end - start, num_items))\n",
    "    title_tags_similarities = np.zeros((end - start, num_items))\n",
    "    for i in range(start, end):\n",
    "        jobroles_tags_similarities[i - start] = items['tag_sets'].apply(lambda x: jaccard_similarity(assigned_users['jobroles_sets'][i], x))\n",
    "        jobroles_title_similarities[i - start] = items['title_sets'].apply(lambda x: jaccard_similarity(assigned_users['jobroles_sets'][i], x))\n",
    "        user_item_similarities = jobroles_tags_similarities,jobroles_title_similarities\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "        #similarities[i - start] = df['tag_sets'].apply(lambda x: jaccard_similarity(df['tag_sets'][i], x))\n",
    "    return user_item_similarities\n",
    "\n",
    "num_items = len(df)\n",
    "\n",
    "# Define the number of parallel processes based on your system's capabilities\n",
    "num_processes = 4\n",
    "\n",
    "# Split the computation into chunks for parallel processing\n",
    "chunk_size = num_users // num_processes\n",
    "chunks = [(i * chunk_size, (i + 1) * chunk_size) for i in range(num_processes - 1)]\n",
    "chunks.append(((num_processes - 1) * chunk_size, num_users))\n",
    "\n",
    "# Initialize a pool of processes\n",
    "with Pool(num_processes) as pool:\n",
    "    results = pool.starmap(compute_similarities_range, chunks)\n",
    "\n",
    "# Combine the results from different processes\n",
    "jaccard_similarity_matrix = np.vstack(results)\n",
    "\n",
    "# Clean up temporary columns\n",
    "df.drop(['tags', 'tag_sets'], axis=1, inplace=True)\n",
    "\n",
    "# The jaccard_similarity_matrix will contain similarity values\n",
    "# where jaccard_similarity_matrix[i, j] represents similarity between item i and item j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
